{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Mining: Assignment 4\n",
    "\n",
    "Please complete all assignments in this notebook. You should submit this notebook, as well as a PDF version (See File > Download as).\n",
    "\n",
    "**Deadline:** Thursday, April 12, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in your names here\n",
    "NAME_STUDENT_1 = \"Bram van der Pol\"\n",
    "NAME_STUDENT_2 = \"Joris van der Heijden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *\n",
    "plt.rcParams['savefig.dpi'] = 100 # This controls the size of your figures\n",
    "# Comment out and restart notebook if you only want the last output of each cell.\n",
    "#InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Backpropagation (6 points)\n",
    "\n",
    "Figure 1 illustrates a simple neural network model.\n",
    "\n",
    "![Figure 1](images/a4_network.png)\n",
    "\n",
    "It has single input $x$, and three layers with respectively one, two, and one neurons. The activation function of the neurons is ReLU. \n",
    "\n",
    "The parameters $w_1$, $w_2$, $w_3$, $w_4$, and $w_5$ (no biases) are initialized to the following values $w_1 = 2, w_2 = 1$, $w_3 = 2$, $w_4 = 4$, and $w_5 = 1$. Implement a single update step of the gradient descent algorithm by hand. Run the update state for the data point $(x=2, y=3)$:\n",
    "\n",
    "The goal is to model the relationship between two continuous variables. The learning rate is set to $0.1$\n",
    "\n",
    "Provide the solution in the following format:\n",
    "\n",
    "- A choice for a loss function \n",
    "- Compute graph for training the neural network\n",
    "- Partial derivative expression for each of the parameters in the model\n",
    "- The update expression for each of the parameters for each of the data-points\n",
    "- The final value of all five parameters after the single step in the gradient descent algorithm\n",
    "\n",
    "The Python code for simple computational graph nodes, as seen in the tutorial session, is provided in the cell below (run the cell to load the code, and again to run the code). Extend the nodes so they can be used to implement the network described above. Implement the network with the same initial weights and the correct learning rate, and verify your hand-made calculations. Add comments to your code or provide a separate description to explain the changes you have made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImplementations of nodes for a computation graph. Each node\\nhas a forward pass and a backward pass function, allowing\\nfor the evaluation and backpropagation of data.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded simple graph nodes\n",
      "4\n",
      "0.9820137900379085\n",
      "0.7275076135036415\n",
      "0.8769681683739503\n",
      "0.9778387307456168\n",
      "Graph 2.0445680994362494\n",
      "w has new value [2.    1.003 2.    4.003 1.004]\n"
     ]
    }
   ],
   "source": [
    "# %load basic_graph.py\n",
    "'''\n",
    "Implementations of nodes for a computation graph. Each node\n",
    "has a forward pass and a backward pass function, allowing\n",
    "for the evaluation and backpropagation of data.\n",
    "'''\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        ''' Feed-forward the result '''\n",
    "        raise NotImplementedError(\"Missing forward-propagation method.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, d):\n",
    "        ''' Back-propagate the error\n",
    "            d is the delta of the subsequent node in the network '''\n",
    "        raise NotImplementedError(\"Missing back-propagation method.\")\n",
    "\n",
    "\n",
    "class ConstantNode(Node):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.output = value\n",
    "\n",
    "    def forward(self):\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        pass\n",
    "\n",
    "\n",
    "class VariableNode(Node):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.output = value\n",
    "\n",
    "    def forward(self):\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.output -= 0.1 * d # Gradient Descent\n",
    "\n",
    "\n",
    "class AdditionNode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = sum([i.forward() for i in self.inputs])\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        for i in self.inputs:\n",
    "            i.backward(d)\n",
    "\n",
    "\n",
    "class MultiplicationNode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = self.inputs[0].forward() * self.inputs[1].forward()\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.inputs[0].backward(d * self.inputs[1].output)\n",
    "        self.inputs[1].backward(d * self.inputs[0].output)\n",
    "\n",
    "\n",
    "class MSENode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = 0.5 * (\n",
    "            self.inputs[0].forward() - self.inputs[1].forward())**2\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.inputs[0].backward(d * (self.inputs[0].output - self.inputs[1].output))\n",
    "        self.inputs[1].backward(d * (self.inputs[1].output - self.inputs[0].output))\n",
    "\n",
    "\n",
    "class SigmoidNode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = 1.0 / (1.0 + math.exp(-self.inputs[0].forward()))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.inputs[0].backward(d * self.output * (1.0 - self.output))\n",
    "\n",
    "class ReLUNode(object):\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError(\"Forward pass for ReLU activation node has not been implemented yet.\")\n",
    "\n",
    "    def backward(self, d):\n",
    "        raise NotImplementedError(\"Backward pass for ReLU activation node has not been implemented yet.\")\n",
    "\n",
    "class TanhNode(object):\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError(\"Forward pass for tanh activation node has not been implemented yet.\")\n",
    "\n",
    "    def backward(self, d):\n",
    "        raise NotImplementedError(\"Backward pass for tanh activation node has not been implemented yet.\")\n",
    "\n",
    "# Example graph as shown in MLP lecture slides\n",
    "class SampleGraph(object):\n",
    "\n",
    "    def __init__(self, x, y, w, b):\n",
    "        ''' x: input\n",
    "            y: expected output\n",
    "            w: initial weight\n",
    "            b: initial bias '''\n",
    "        self.w = VariableNode(w)\n",
    "        self.b = VariableNode(b)\n",
    "        self.graph = MSENode([\n",
    "            AdditionNode([\n",
    "                MultiplicationNode([\n",
    "                    ConstantNode(x),\n",
    "                    self.w\n",
    "                ]),\n",
    "                MultiplicationNode([\n",
    "                    self.b,\n",
    "                    ConstantNode(1)\n",
    "                ])\n",
    "            ]),\n",
    "            ConstantNode(y)\n",
    "        ])\n",
    "\n",
    "    def forward(self):\n",
    "        return self.graph.forward()\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.graph.backward(d)\n",
    "\n",
    "\n",
    "class Neuron(Node):\n",
    "\n",
    "    def __init__(self, inputs, weights, activation):\n",
    "        ''' weights: list of initial weights, same length as inputs '''\n",
    "        self.inputs = inputs\n",
    "        # Initialize a weight for each input\n",
    "        self.weights = [VariableNode(weight) for weight in weights]\n",
    "        # Neurons normally have a bias, ignore for this assignment\n",
    "        #self.bias = VariableNode(bias, \"b\")\n",
    "\n",
    "        # Multiplication node for each pair of inputs and weights\n",
    "        mults = [MultiplicationNode([i, w]) for i, w, in zip(self.inputs, self.weights)]\n",
    "        # Neurons normally have a bias, ignore for this assignment\n",
    "        #mults.append(MultiplicationNode([self.bias, ConstantNode(1)]))\n",
    "\n",
    "        # Sum all multiplication results\n",
    "        added = AdditionNode(mults)\n",
    "\n",
    "        # Apply activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.graph = SigmoidNode([added])\n",
    "        elif activation == 'relu':\n",
    "            self.graph = ReLUNode([added])\n",
    "        elif activation == 'tanh':\n",
    "            self.graph = TanhNode([added])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function.\")\n",
    "\n",
    "    def forward(self):\n",
    "        return self.graph.forward()\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.graph.backward(d)\n",
    "\n",
    "    def set_weights(self, new_weights):\n",
    "        for i in len(new_weights):\n",
    "            self.weights[i].output = new_weights[i]\n",
    "\n",
    "    def get_weights(self):\n",
    "        return [weight.output for weight in self.weights]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Loaded simple graph nodes\")\n",
    "\n",
    "    # Example network\n",
    "    #sg = SampleGraph(2, 2, 2, 1)\n",
    "    #prediction = sg.forward()\n",
    "    #print(\"Initial prediction is\", prediction)\n",
    "    #sg.backward(1)\n",
    "    #print(\"w has new value\", sg.w.output)\n",
    "    #print(\"b has new value\", sg.b.output)\n",
    "\n",
    "    # Run your network here\n",
    "class MyGraph(object):\n",
    "\n",
    "    def __init__(self, x, y, weigths):\n",
    "        ''' x: input\n",
    "            y: expected output\n",
    "            w: initial weight\n",
    "            b: initial bias '''\n",
    "                       \n",
    "        self.weights = [VariableNode(weight) for weight in weigths]\n",
    "        \n",
    "        self.z1 = MultiplicationNode([ConstantNode(x),self.weights[0]])\n",
    "        self.n1 = SigmoidNode([self.z1])\n",
    "        self.z2 = MultiplicationNode([self.n1, self.weights[1]])\n",
    "        self.n2 = SigmoidNode([self.z2])\n",
    "        self.z3 = MultiplicationNode([self.n1, self.weights[2]])\n",
    "        self.n3 = SigmoidNode([self.z3])\n",
    "        self.z4 = AdditionNode([MultiplicationNode([\n",
    "                             self.n2,\n",
    "                             self.weights[3]]),\n",
    "                             MultiplicationNode([\n",
    "                             self.n3,\n",
    "                             self.weights[4]])])\n",
    "        self.n4 = SigmoidNode([self.z4])\n",
    "        self.graph = MSENode([self.n4,ConstantNode(y)])\n",
    "\n",
    "    def forward(self):\n",
    "        return self.graph.forward()\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.graph.backward(d)\n",
    "    def set_weights(self, new_weights):\n",
    "        for i in len(new_weights):\n",
    "            self.weights[i].output = new_weights[i]\n",
    "\n",
    "    def get_weights(self):\n",
    "        return [weight.output for weight in self.weights]\n",
    "\n",
    "x=2\n",
    "y=3\n",
    "w1=2\n",
    "w2=1\n",
    "w3=2\n",
    "w4=4\n",
    "w5=1\n",
    "\n",
    "\n",
    "sg = MyGraph(x, y,[w1,w2,w3,w4,w5])\n",
    "\n",
    "Error1 = sg.forward()\n",
    "\n",
    "print(sg.z1.output)\n",
    "print(sg.n1.output)\n",
    "print(sg.n2.output)\n",
    "print(sg.n3.output)\n",
    "print(sg.n4.output)\n",
    "print(\"Graph\",sg.graph.output)\n",
    "\n",
    "sg.backward(sg.n4.output)\n",
    "\n",
    "new_weights=sg.get_weights()\n",
    "print(\"w has new value\", np.transpose(new_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Deep Models (3 points)\n",
    "\n",
    "The model in the example code below performs poorly as its depth increases. Train this model on the MNIST digit detection task. \n",
    "\n",
    "Examine its training performance by gradually increasing its depth:\n",
    "- Set the depth to 1 hidden layer\n",
    "- Set the depth to 2 hidden layers\n",
    "- Set the depth to 3 hidden layers\n",
    "\n",
    "Modify the model such that you improve its performance when its depth increases. Train the new model again for the different depths:\n",
    "- Set the depth to 1 hidden layer\n",
    "- Set the depth to 2 hidden layers\n",
    "- Set the depth to 3 hidden layers\n",
    "\n",
    "Submit an explanation for the limitation of the original model. Explain your modification. \n",
    "Submit your code and 6 plots (can be overlaid) for the training performance of both models with different depths. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (You don't need to change this part of the code)\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, Dense, TimeDistributed, Activation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# (You don't need to change this part of the code)\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this parameter to change the depth of the model\n",
    "number_hidden_layers = 2  # Number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nwhile number_hidden_layers > 1:\\n    model.add(Dense(512))\\n    model.add(Activation('sigmoid'))\\n    model.add(Dropout(0.2))\\n    number_hidden_layers -= 1\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_53 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=(784,), activation='sigmoid'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "\n",
    "if number_hidden_layers > 1:\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.15))\n",
    "if number_hidden_layers>2:\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.15))\n",
    "'''\n",
    "\n",
    "\n",
    "while number_hidden_layers > 1:\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    number_hidden_layers -= 1\n",
    "'''\n",
    "\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 1.9890 - acc: 0.3821 - val_loss: 1.6158 - val_acc: 0.7009\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 1.4338 - acc: 0.6643 - val_loss: 1.1652 - val_acc: 0.7952\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 1.0925 - acc: 0.7467 - val_loss: 0.9063 - val_acc: 0.8330\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.8925 - acc: 0.7847 - val_loss: 0.7524 - val_acc: 0.8467\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.7696 - acc: 0.8083 - val_loss: 0.6578 - val_acc: 0.8582\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.6917 - acc: 0.8202 - val_loss: 0.5934 - val_acc: 0.8651\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.6343 - acc: 0.8325 - val_loss: 0.5464 - val_acc: 0.8718\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.5955 - acc: 0.8394 - val_loss: 0.5123 - val_acc: 0.8754\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.5627 - acc: 0.8467 - val_loss: 0.4841 - val_acc: 0.8798\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.5369 - acc: 0.8502 - val_loss: 0.4622 - val_acc: 0.8834\n",
      "Test score: 0.4621768421173096\n",
      "Test accuracy: 0.8834\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2bf244eeb70>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2bf242f6ac8>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'model accuracy')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'accuracy')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'epoch')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2bf244d6c18>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/pdf": "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\n",
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bf242f6b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training (You don't need to change this part of the code)\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Calculator (6 points)\n",
    "\n",
    "During the lectures you have seen a CNN model that can be successfully trained to classify the MNIST images. You have also seen how a RNN model that can be trained to implement addition of two numbers. You now need to build a model that is a combination of convolutional layers and recurrent cells. \n",
    "\n",
    "Using the KERAS library, design and train a model that produces a sum of a sequence of MNIST images. More specifically, the model should input a sequence of 10 images and compute the cumulative sum of the digits represented by the images.\n",
    "\n",
    "For example:\n",
    "\n",
    "Input 1: ![294](images/a3ex1.png)\n",
    "\n",
    "Output 1: 46\n",
    "\n",
    "Input 2: ![61](images/a3ex2.png)\n",
    "\n",
    "Output 2: 43\n",
    "\n",
    "Your solutions should include:\n",
    "- Python code that formats the MNIST dataset such that it can be used for traning and testing your model\n",
    "- Implementation in keras of your model (for training and testing)\n",
    "- Performance on the model on test data\n",
    "- Justification (in text) of your decisions for the model architecture (type of layers, activation functions, loss function, regularization and training hyperparameters)\n",
    "\n",
    "Note: Use the 60000/10000 train/test split of the MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, Dense, TimeDistributed, Activation\n",
    "#from keras import layers \n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n",
    "import numpy as np\n",
    "\n",
    "# Training parameters\n",
    "num_classes = 90\n",
    "num_images = 10\n",
    "\n",
    "TRAINING_SIZE = 6000\n",
    "TESTING_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) train samples\n",
      "(10000, 28, 28) test samples\n",
      "(12000, 10, 784) train samples\n",
      "(1000, 10, 784) test samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd24a31f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Python code that formats the MNIST dataset such that it can be used for traning and testing your model\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# The data, shuffled and split between train and test sets\n",
    "(x_train_in, y_train_in), (x_test_in, y_test_in) = mnist.load_data()\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x_train_in[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\" {}\".format(y_train_in[i]))\n",
    "\n",
    "x_train_in = x_train_in.astype('float32')\n",
    "x_test_in = x_test_in.astype('float32')\n",
    "x_train_in /= 255\n",
    "x_test_in /= 255\n",
    "    \n",
    "print(x_train_in.shape, 'train samples')\n",
    "print(x_test_in.shape, 'test samples')\n",
    "\n",
    "# build training set\n",
    "x_train = np.zeros((TRAINING_SIZE*2, num_images,img_cols * img_rows), dtype=np.float)\n",
    "y_train = np.zeros(TRAINING_SIZE*2)\n",
    "for j in range(TRAINING_SIZE):    \n",
    "\n",
    "    #just split training images in pairs of 10 images\n",
    "    for i in range(num_images):\n",
    "\n",
    "        index = (num_images*j)+i\n",
    "        x_temp = x_train_in[index] # select next image\n",
    "        x_temp = x_temp.reshape(1, img_cols * img_rows) # flatten the image\n",
    "        x_train[j][i]= x_temp\n",
    "        y_train[j] += y_train_in[index]\n",
    "        \n",
    "    # make random combinations of training images  \n",
    "    for i in range(num_images):\n",
    "\n",
    "        index = np.random.randint(0, TRAINING_SIZE-1)\n",
    "        x_temp = x_train_in[index] # select next image\n",
    "        x_temp = x_temp.reshape(1, img_cols * img_rows) # flatten the image\n",
    "        x_train[j+TRAINING_SIZE][i]= x_temp\n",
    "        y_train[j+TRAINING_SIZE] += y_train_in[index]      \n",
    "\n",
    "# build test set\n",
    "x_test = np.zeros((TESTING_SIZE, num_images, img_cols * img_rows), dtype=np.float)\n",
    "y_test = np.zeros(TESTING_SIZE)\n",
    "for j in range(TESTING_SIZE):    \n",
    "    \n",
    "    for i in range(num_images):\n",
    "        #x_temp = x_train_in[np.random.randint(0, TRAINING_SIZE)] # select a random image\n",
    "        index = (num_images*j)+i\n",
    "        x_temp = x_test_in[index] # select next image\n",
    "        x_temp = x_temp.reshape(1, img_cols * img_rows) # flatten the image\n",
    "        x_test[j][i] = x_temp\n",
    "        y_test[j]+= y_test_in[index]\n",
    "    \n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_test.shape, 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 10, 784)           615440    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 10, 784)           0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10, 128)           100480    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10, 10)            1290      \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 2, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 2, 128)            117248    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 2, 10)             1290      \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 90)                1890      \n",
      "=================================================================\n",
      "Total params: 837,638\n",
      "Trainable params: 837,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Implementation in keras of your model (for training and testing)\n",
    "\n",
    "RNN = layers.LSTM\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "\n",
    "# use convolution to look at each image seperately, 10 digits -> 10 dimensions?\n",
    "#model.add(Conv1D(10,img_cols * img_rows, strides=img_cols * img_rows,padding='same',input_shape=x_train.shape[1:])) \n",
    "\n",
    "model.add(Dense(784, input_shape=(10,784), activation='relu'))\n",
    "model.add(Dropout(0.2))    # lowered dropout to reduce overfitting\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(10, activation='sigmoid')) # get a value between 1 - 10 for each of the 10 images\n",
    "          \n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(layers.RepeatVector(2)) # maximum sum length 2 (90)\n",
    "\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    model.add(RNN(128, return_sequences=True))\n",
    "    \n",
    "# Apply a dense layer to the every temporal slice of an input. \n",
    "model.add(layers.TimeDistributed(layers.Dense(10))) # need to sum 10 elements\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(90, activation='softmax')) # determine sum 'class'\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 1000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 4s 314us/step - loss: 3.9366 - acc: 0.0387 - val_loss: 3.6356 - val_acc: 0.1030\n",
      "Epoch 2/80\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 3.6564 - acc: 0.0402 - val_loss: 3.6099 - val_acc: 0.1030\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 3.6304 - acc: 0.0431 - val_loss: 3.5572 - val_acc: 0.0720\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 3.3840 - acc: 0.0588 - val_loss: 3.0648 - val_acc: 0.0750\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 2.8716 - acc: 0.1040 - val_loss: 2.8289 - val_acc: 0.1130\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 2.5780 - acc: 0.1483 - val_loss: 2.5851 - val_acc: 0.1520\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 2.3726 - acc: 0.1842 - val_loss: 2.4448 - val_acc: 0.1620\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 2.2056 - acc: 0.2188 - val_loss: 2.2835 - val_acc: 0.2540\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 2.0891 - acc: 0.2522 - val_loss: 2.2495 - val_acc: 0.2640\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 1.9731 - acc: 0.2908 - val_loss: 2.1987 - val_acc: 0.2790\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 1.8687 - acc: 0.3252 - val_loss: 2.1859 - val_acc: 0.3320\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 1.7917 - acc: 0.3550 - val_loss: 2.1394 - val_acc: 0.3440\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 1.7112 - acc: 0.3944 - val_loss: 2.1167 - val_acc: 0.3080\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 1.6424 - acc: 0.4147 - val_loss: 2.0529 - val_acc: 0.3940\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 1.5769 - acc: 0.4448 - val_loss: 2.0914 - val_acc: 0.3990\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 1.5096 - acc: 0.4812 - val_loss: 2.1014 - val_acc: 0.3960\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 1.4430 - acc: 0.5105 - val_loss: 2.1168 - val_acc: 0.4540\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 1.3963 - acc: 0.5235 - val_loss: 2.0466 - val_acc: 0.4970\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 1.3543 - acc: 0.5487 - val_loss: 2.0931 - val_acc: 0.5020\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 1.3052 - acc: 0.5832 - val_loss: 2.0889 - val_acc: 0.5000\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 1.2635 - acc: 0.6109 - val_loss: 2.1440 - val_acc: 0.5020\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 1.1988 - acc: 0.6493 - val_loss: 2.1201 - val_acc: 0.4990\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 1.1701 - acc: 0.6702 - val_loss: 2.1209 - val_acc: 0.5460\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 2s 202us/step - loss: 1.1354 - acc: 0.6836 - val_loss: 2.1518 - val_acc: 0.5040\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 1.0952 - acc: 0.7037 - val_loss: 2.1630 - val_acc: 0.5920\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 1.0635 - acc: 0.6991 - val_loss: 2.1665 - val_acc: 0.5820\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 1.0102 - acc: 0.7373 - val_loss: 2.1340 - val_acc: 0.5950\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 2s 202us/step - loss: 0.9924 - acc: 0.7427 - val_loss: 2.2025 - val_acc: 0.5950\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.9528 - acc: 0.7634 - val_loss: 2.2559 - val_acc: 0.6160\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 2s 203us/step - loss: 0.9110 - acc: 0.7856 - val_loss: 2.1876 - val_acc: 0.6170\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.9008 - acc: 0.7842 - val_loss: 2.2277 - val_acc: 0.6240\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 0.8841 - acc: 0.7883 - val_loss: 2.1335 - val_acc: 0.6500\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.8431 - acc: 0.8028 - val_loss: 2.2022 - val_acc: 0.6560\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.8249 - acc: 0.8076 - val_loss: 2.2495 - val_acc: 0.6270\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 2s 202us/step - loss: 0.7834 - acc: 0.8202 - val_loss: 2.3094 - val_acc: 0.6500\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.7957 - acc: 0.8182 - val_loss: 2.2185 - val_acc: 0.6560\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 2s 205us/step - loss: 0.7596 - acc: 0.8282 - val_loss: 2.2195 - val_acc: 0.6440\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 2s 207us/step - loss: 0.7621 - acc: 0.8257 - val_loss: 2.2382 - val_acc: 0.6630\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 2s 203us/step - loss: 0.7364 - acc: 0.8354 - val_loss: 2.2572 - val_acc: 0.6740\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 2s 203us/step - loss: 0.7037 - acc: 0.8458 - val_loss: 2.2343 - val_acc: 0.6610\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 2s 202us/step - loss: 0.6682 - acc: 0.8593 - val_loss: 2.2298 - val_acc: 0.6830\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.6795 - acc: 0.8583 - val_loss: 2.1769 - val_acc: 0.6860\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 3s 214us/step - loss: 0.6335 - acc: 0.8686 - val_loss: 2.1806 - val_acc: 0.6700\n",
      "Epoch 44/80\n",
      "12000/12000 [==============================] - 2s 206us/step - loss: 0.6411 - acc: 0.8661 - val_loss: 2.2809 - val_acc: 0.6990\n",
      "Epoch 45/80\n",
      "12000/12000 [==============================] - 3s 214us/step - loss: 0.6325 - acc: 0.8658 - val_loss: 2.2788 - val_acc: 0.6860\n",
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 3s 226us/step - loss: 0.6325 - acc: 0.8716 - val_loss: 2.2298 - val_acc: 0.6740\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 3s 213us/step - loss: 0.6071 - acc: 0.8742 - val_loss: 2.3016 - val_acc: 0.6720\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 3s 222us/step - loss: 0.6044 - acc: 0.8728 - val_loss: 2.3034 - val_acc: 0.6690\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 3s 221us/step - loss: 0.6211 - acc: 0.8724 - val_loss: 2.2636 - val_acc: 0.6900\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 3s 223us/step - loss: 0.5762 - acc: 0.8841 - val_loss: 2.2855 - val_acc: 0.6950\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 3s 219us/step - loss: 0.5968 - acc: 0.8811 - val_loss: 2.1453 - val_acc: 0.7110\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 2s 204us/step - loss: 0.5528 - acc: 0.8816 - val_loss: 2.2930 - val_acc: 0.6930\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 2s 201us/step - loss: 0.5424 - acc: 0.8897 - val_loss: 2.3006 - val_acc: 0.6940\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.5663 - acc: 0.8867 - val_loss: 2.2625 - val_acc: 0.6810\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 2s 205us/step - loss: 0.5453 - acc: 0.8911 - val_loss: 2.1201 - val_acc: 0.7060\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 2s 202us/step - loss: 0.5150 - acc: 0.9002 - val_loss: 2.3212 - val_acc: 0.6890\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 2s 201us/step - loss: 0.5185 - acc: 0.8977 - val_loss: 2.1707 - val_acc: 0.7100\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 2s 201us/step - loss: 0.5143 - acc: 0.8989 - val_loss: 2.4042 - val_acc: 0.6820\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 2s 206us/step - loss: 0.5328 - acc: 0.8935 - val_loss: 2.2361 - val_acc: 0.7090\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.4851 - acc: 0.9028 - val_loss: 2.0359 - val_acc: 0.7160\n",
      "Epoch 61/80\n",
      "12000/12000 [==============================] - 3s 211us/step - loss: 0.4791 - acc: 0.9018 - val_loss: 2.0959 - val_acc: 0.7210\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 3s 210us/step - loss: 0.4584 - acc: 0.9066 - val_loss: 2.1887 - val_acc: 0.7010\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 3s 231us/step - loss: 0.4670 - acc: 0.9058 - val_loss: 2.1572 - val_acc: 0.7220\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 3s 224us/step - loss: 0.4742 - acc: 0.9081 - val_loss: 2.1978 - val_acc: 0.7160\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 2s 207us/step - loss: 0.4518 - acc: 0.9113 - val_loss: 2.2384 - val_acc: 0.7140\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.4546 - acc: 0.9081 - val_loss: 2.1008 - val_acc: 0.7270\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 3s 218us/step - loss: 0.4761 - acc: 0.9083 - val_loss: 2.1410 - val_acc: 0.7200\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 2s 204us/step - loss: 0.4222 - acc: 0.9173 - val_loss: 2.0858 - val_acc: 0.7240\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 3s 209us/step - loss: 0.4331 - acc: 0.9160 - val_loss: 2.1318 - val_acc: 0.7120\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 3s 215us/step - loss: 0.4093 - acc: 0.9196 - val_loss: 2.1732 - val_acc: 0.7160\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.4561 - acc: 0.9102 - val_loss: 2.1401 - val_acc: 0.7230\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 2s 203us/step - loss: 0.4210 - acc: 0.9187 - val_loss: 2.2374 - val_acc: 0.7190\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 2s 202us/step - loss: 0.4707 - acc: 0.9055 - val_loss: 2.1733 - val_acc: 0.7270\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.4151 - acc: 0.9188 - val_loss: 2.1724 - val_acc: 0.7150\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.3799 - acc: 0.9266 - val_loss: 2.1393 - val_acc: 0.7350\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 2s 203us/step - loss: 0.4090 - acc: 0.9185 - val_loss: 2.0775 - val_acc: 0.7350\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 2s 201us/step - loss: 0.3896 - acc: 0.9244 - val_loss: 2.2032 - val_acc: 0.7190\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 2s 202us/step - loss: 0.4112 - acc: 0.9208 - val_loss: 2.1587 - val_acc: 0.7310\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 2s 203us/step - loss: 0.4272 - acc: 0.9137 - val_loss: 2.2666 - val_acc: 0.7080\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.3892 - acc: 0.9233 - val_loss: 2.1916 - val_acc: 0.7240\n",
      "Test loss: 2.191581526756287\n",
      "Test accuracy: 0.724\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=80,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test)) \n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Provide your justification here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
