{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Mining: Assignment 4\n",
    "\n",
    "Please complete all assignments in this notebook. You should submit this notebook, as well as a PDF version (See File > Download as).\n",
    "\n",
    "**Deadline:** Thursday, April 12, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please fill in your names here\n",
    "NAME_STUDENT_1 = \"\"\n",
    "NAME_STUDENT_2 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *\n",
    "plt.rcParams['savefig.dpi'] = 100 # This controls the size of your figures\n",
    "# Comment out and restart notebook if you only want the last output of each cell.\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Backpropagation (6 points)\n",
    "\n",
    "Figure 1 illustrates a simple neural network model.\n",
    "\n",
    "![Figure 1](images/a4_network.png)\n",
    "\n",
    "It has single input $x$, and three layers with respectively one, two, and one neurons. The activation function of the neurons is ReLU. \n",
    "\n",
    "The parameters $w_1$, $w_2$, $w_3$, $w_4$, and $w_5$ (no biases) are initialized to the following values $w_1 = 2, w_2 = 1$, $w_3 = 2$, $w_4 = 4$, and $w_5 = 1$. Implement a single update step of the gradient descent algorithm by hand. Run the update state for the data point $(x=2, y=3)$:\n",
    "\n",
    "The goal is to model the relationship between two continuous variables. The learning rate is set to $0.1$\n",
    "\n",
    "Provide the solution in the following format:\n",
    "\n",
    "- A choice for a loss function \n",
    "- Compute graph for training the neural network\n",
    "- Partial derivative expression for each of the parameters in the model\n",
    "- The update expression for each of the parameters for each of the data-points\n",
    "- The final value of all five parameters after the single step in the gradient descent algorithm\n",
    "\n",
    "The Python code for simple computational graph nodes, as seen in the tutorial session, is provided in the cell below (run the cell to load the code, and again to run the code). Extend the nodes so they can be used to implement the network described above. Implement the network with the same initial weights and the correct learning rate, and verify your hand-made calculations. Add comments to your code or provide a separate description to explain the changes you have made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load basic_graph.py\n",
    "'''\n",
    "Implementations of nodes for a computation graph. Each node\n",
    "has a forward pass and a backward pass function, allowing\n",
    "for the evaluation and backpropagation of data.\n",
    "'''\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        ''' Feed-forward the result '''\n",
    "        raise NotImplementedError(\"Missing forward-propagation method.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, d):\n",
    "        ''' Back-propagate the error\n",
    "            d is the delta of the subsequent node in the network '''\n",
    "        raise NotImplementedError(\"Missing back-propagation method.\")\n",
    "\n",
    "\n",
    "class ConstantNode(Node):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.output = value\n",
    "\n",
    "    def forward(self):\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        pass\n",
    "\n",
    "\n",
    "class VariableNode(Node):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.output = value\n",
    "\n",
    "    def forward(self):\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.output -= 0.1 * d # Gradient Descent\n",
    "\n",
    "\n",
    "class AdditionNode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = sum([i.forward() for i in self.inputs])\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        for i in self.inputs:\n",
    "            i.backward(d)\n",
    "\n",
    "\n",
    "class MultiplicationNode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = self.inputs[0].forward() * self.inputs[1].forward()\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.inputs[0].backward(d * self.inputs[1].output)\n",
    "        self.inputs[1].backward(d * self.inputs[0].output)\n",
    "\n",
    "\n",
    "class MSENode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = 0.5 * (\n",
    "            self.inputs[0].forward() - self.inputs[1].forward())**2\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.inputs[0].backward(d * (self.inputs[0].output - self.inputs[1].output))\n",
    "        self.inputs[1].backward(d * (self.inputs[1].output - self.inputs[0].output))\n",
    "\n",
    "\n",
    "class SigmoidNode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = 1.0 / (1.0 + math.exp(-self.inputs[0].forward()))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.inputs[0].backward(d * self.output * (1.0 - self.output))\n",
    "\n",
    "class ReLUNode(object):\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError(\"Forward pass for ReLU activation node has not been implemented yet.\")\n",
    "\n",
    "    def backward(self, d):\n",
    "        raise NotImplementedError(\"Backward pass for ReLU activation node has not been implemented yet.\")\n",
    "\n",
    "class TanhNode(object):\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError(\"Forward pass for tanh activation node has not been implemented yet.\")\n",
    "\n",
    "    def backward(self, d):\n",
    "        raise NotImplementedError(\"Backward pass for tanh activation node has not been implemented yet.\")\n",
    "\n",
    "# Example graph as shown in MLP lecture slides\n",
    "class SampleGraph(object):\n",
    "\n",
    "    def __init__(self, x, y, w, b):\n",
    "        ''' x: input\n",
    "            y: expected output\n",
    "            w: initial weight\n",
    "            b: initial bias '''\n",
    "        self.w = VariableNode(w)\n",
    "        self.b = VariableNode(b)\n",
    "        self.graph = MSENode([\n",
    "            AdditionNode([\n",
    "                MultiplicationNode([\n",
    "                    ConstantNode(x),\n",
    "                    self.w\n",
    "                ]),\n",
    "                MultiplicationNode([\n",
    "                    self.b,\n",
    "                    ConstantNode(1)\n",
    "                ])\n",
    "            ]),\n",
    "            ConstantNode(y)\n",
    "        ])\n",
    "\n",
    "    def forward(self):\n",
    "        return self.graph.forward()\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.graph.backward(d)\n",
    "\n",
    "\n",
    "class Neuron(Node):\n",
    "\n",
    "    def __init__(self, inputs, weights, activation):\n",
    "        ''' weights: list of initial weights, same length as inputs '''\n",
    "        self.inputs = inputs\n",
    "        # Initialize a weight for each input\n",
    "        self.weights = [VariableNode(weight) for weight in weights]\n",
    "        # Neurons normally have a bias, ignore for this assignment\n",
    "        #self.bias = VariableNode(bias, \"b\")\n",
    "\n",
    "        # Multiplication node for each pair of inputs and weights\n",
    "        mults = [MultiplicationNode([i, w]) for i, w, in zip(self.inputs, self.weights)]\n",
    "        # Neurons normally have a bias, ignore for this assignment\n",
    "        #mults.append(MultiplicationNode([self.bias, ConstantNode(1)]))\n",
    "\n",
    "        # Sum all multiplication results\n",
    "        added = AdditionNode(mults)\n",
    "\n",
    "        # Apply activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.graph = SigmoidNode([added])\n",
    "        elif activation == 'relu':\n",
    "            self.graph = ReLUNode([added])\n",
    "        elif activation == 'tanh':\n",
    "            self.graph = TanhNode([added])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function.\")\n",
    "\n",
    "    def forward(self):\n",
    "        return self.graph.forward()\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.graph.backward(d)\n",
    "\n",
    "    def set_weights(self, new_weights):\n",
    "        for i in len(new_weights):\n",
    "            self.weights[i].output = new_weights[i]\n",
    "\n",
    "    def get_weights(self):\n",
    "        return [weight.output for weight in self.weights]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Loaded simple graph nodes\")\n",
    "\n",
    "    # Example network\n",
    "    #sg = SampleGraph(2, 2, 2, 1)\n",
    "    #prediction = sg.forward()\n",
    "    #print(\"Initial prediction is\", prediction)\n",
    "    #sg.backward(1)\n",
    "    #print(\"w has new value\", sg.w.output)\n",
    "    #print(\"b has new value\", sg.b.output)\n",
    "\n",
    "    # Run your network here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Deep Models (3 points)\n",
    "\n",
    "The model in the example code below performs poorly as its depth increases. Train this model on the MNIST digit detection task. \n",
    "\n",
    "Examine its training performance by gradually increasing its depth:\n",
    "- Set the depth to 1 hidden layer\n",
    "- Set the depth to 2 hidden layers\n",
    "- Set the depth to 3 hidden layers\n",
    "\n",
    "Modify the model such that you improve its performance when its depth increases. Train the new model again for the different depths:\n",
    "- Set the depth to 1 hidden layer\n",
    "- Set the depth to 2 hidden layers\n",
    "- Set the depth to 3 hidden layers\n",
    "\n",
    "Submit an explanation for the limitation of the original model. Explain your modification. \n",
    "Submit your code and 6 plots (can be overlaid) for the training performance of both models with different depths. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (You don't need to change this part of the code)\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 100\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (You don't need to change this part of the code)\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this parameter to change the depth of the model\n",
    "number_hidden_layers = 1  # Number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,), activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "while number_hidden_layers > 1:\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    number_hidden_layers -= 1\n",
    "\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training (You don't need to change this part of the code)\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Calculator (6 points)\n",
    "\n",
    "During the lectures you have seen a CNN model that can be successfully trained to classify the MNIST images. You have also seen how a RNN model that can be trained to implement addition of two numbers. You now need to build a model that is a combination of convolutional layers and recurrent cells. \n",
    "\n",
    "Using the KERAS library, design and train a model that produces a sum of a sequence of MNIST images. More specifically, the model should input a sequence of 10 images and compute the cumulative sum of the digits represented by the images.\n",
    "\n",
    "For example:\n",
    "\n",
    "Input 1: ![294](images/a3ex1.png)\n",
    "\n",
    "Output 1: 46\n",
    "\n",
    "Input 2: ![61](images/a3ex2.png)\n",
    "\n",
    "Output 2: 43\n",
    "\n",
    "Your solutions should include:\n",
    "- Python code that formats the MNIST dataset such that it can be used for traning and testing your model\n",
    "- Implementation in keras of your model (for training and testing)\n",
    "- Performance on the model on test data\n",
    "- Justification (in text) of your decisions for the model architecture (type of layers, activation functions, loss function, regularization and training hyperparameters)\n",
    "\n",
    "Note: Use the 60000/10000 train/test split of the MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Provide your solution here\n",
    "\n",
    "# Imports\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n",
    "import numpy as np\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "TRAINING_SIZE = 6000\n",
    "TESTING_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) train samples\n",
      "(10000, 28, 28) test samples\n",
      "(6000, 10, 784) train samples\n",
      "(1000, 10, 784) test samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17d46b23dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data preparation\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# The data, shuffled and split between train and test sets\n",
    "(x_train_in, y_train_in), (x_test_in, y_test_in) = mnist.load_data()\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x_train_in[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\" {}\".format(y_train_in[i]))\n",
    "\n",
    "x_train_in = x_train_in.astype('float32')\n",
    "x_test_in = x_test_in.astype('float32')\n",
    "x_train_in /= 255\n",
    "x_test_in /= 255\n",
    "    \n",
    "print(x_train_in.shape, 'train samples')\n",
    "print(x_test_in.shape, 'test samples')\n",
    "\n",
    "# build training set\n",
    "x_train = np.zeros((TRAINING_SIZE, 10,img_cols * img_rows), dtype=np.float)\n",
    "y_train = np.zeros((TRAINING_SIZE, 10))\n",
    "for j in range(TRAINING_SIZE):    \n",
    "\n",
    "    for i in range(10):\n",
    "        #x_temp = x_train_in[np.random.randint(0, TRAINING_SIZE)] # select a random image\n",
    "        index = (10*j)+i\n",
    "        x_temp = x_train_in[index] # select next image\n",
    "        x_temp = x_temp.reshape(1, img_cols * img_rows) # flatten the image\n",
    "        x_train[j][i]= x_temp\n",
    "        y_train[j] = y_train_in[index]\n",
    "\n",
    "# build test set\n",
    "x_test = np.zeros((TESTING_SIZE, 10, img_cols * img_rows), dtype=np.float)\n",
    "y_test = np.zeros((TESTING_SIZE,10))\n",
    "for j in range(TESTING_SIZE):    \n",
    "    \n",
    "    for i in range(10):\n",
    "        #x_temp = x_train_in[np.random.randint(0, TRAINING_SIZE)] # select a random image\n",
    "        index = (10*j)+i\n",
    "        x_temp = x_test_in[index] # select next image\n",
    "        x_temp = x_temp.reshape(1, img_cols * img_rows) # flatten the image\n",
    "        x_test[j][i] = x_temp\n",
    "        y_test[j] = y_test_in[index]\n",
    "    \n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_test.shape, 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 10, 10)            6146570   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10, 128)           1408      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10, 100)           12900     \n",
      "=================================================================\n",
      "Total params: 6,160,878\n",
      "Trainable params: 6,160,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Conv1D(10, 784, padding='same',input_shape=x_train.shape[1:])) #\n",
    "\n",
    "model.add(Dense(128, activation='relu', ))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "6000/6000 [==============================] - 25s 4ms/step - loss: 2.3202 - acc: 0.2038 - val_loss: 2.0148 - val_acc: 0.2646\n",
      "Epoch 2/120\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.8182 - acc: 0.3637 - val_loss: 1.9840 - val_acc: 0.2905\n",
      "Epoch 3/120\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.6060 - acc: 0.4462 - val_loss: 2.1930 - val_acc: 0.2910\n",
      "Epoch 4/120\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.4726 - acc: 0.4961 - val_loss: 1.8643 - val_acc: 0.3667\n",
      "Epoch 5/120\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.3606 - acc: 0.5399 - val_loss: 2.0173 - val_acc: 0.3660\n",
      "Epoch 6/120\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.2893 - acc: 0.5629 - val_loss: 1.7069 - val_acc: 0.4361\n",
      "Epoch 7/120\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.2142 - acc: 0.5905 - val_loss: 1.8928 - val_acc: 0.4019\n",
      "Epoch 8/120\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.1687 - acc: 0.6083 - val_loss: 1.7756 - val_acc: 0.4447\n",
      "Epoch 9/120\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.1282 - acc: 0.6232 - val_loss: 2.0075 - val_acc: 0.4197\n",
      "Epoch 10/120\n",
      "4134/6000 [===================>..........] - ETA: 6s - loss: 1.0925 - acc: 0.6337"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Provide your justification here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
