{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Foundations of Data Mining: Assignment 2\n",
    "\n",
    "Please complete all assignments in this notebook. You should submit this notebook, as well as a PDF version (See File > Download as)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in your names here\n",
    "NAME_STUDENT_1 = \"Bram van der Pol\"\n",
    "NAME_STUDENT_2 = \"Joris van der Heijden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>html, body{overflow: visible !important} .CodeMirror{min-width:105% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:140%; line-height:1.2; overflow: visible;} .output_subarea pre{width:110%}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *\n",
    "plt.rcParams['savefig.dpi'] = 100 # This controls the size of your figures\n",
    "# Comment out and restart notebook if you only want the last output of each cell.\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "HTML('''<style>html, body{overflow: visible !important} .CodeMirror{min-width:105% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:140%; line-height:1.2; overflow: visible;} .output_subarea pre{width:110%}</style>''') # For slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Bananas (4 points (2+2))\n",
    "We will first explore SVM kernels and hyperparameters on an artificial dataset representing multiple banana shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import *\n",
    "bananas = oml.datasets.get_dataset(1460) # Download banana data\n",
    "X, y = bananas.get_data(target=bananas.default_target_attribute);\n",
    "#X_del, X, y_del, y = train_test_split(X, y, test_size=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=1/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 . Evaluate how well an SVM classifier can fit the data. \n",
    "\n",
    "- Use a linear, polynomial and radial basis function (RBF) kernel, using their default hyperparameters. Evaluate the performance of each kernel using the test set and AUC. Which one works best? \n",
    "\n",
    "The polynomial works best, see figure\n",
    "- Visualize the results using the visualization code also used in class (under mglearn/plot_svm.py > plot_svm_kernels). Also show the AUC score and the number of support vectors. Explain intuitively how well the data is fitted, why the kernel is (not) able to fit the data, whether it is under- or overfitting, etc.\n",
    "\n",
    "The data is not linear neither based on a radial basis (for sine cosine based data). All kernels are not overfitting, because the radii are relatively large and no islands appear in the contour plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3975, 2)\n",
      "(5300,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#C = 1.0  # SVM regularization parameter\n",
    "models = (svm.SVC(kernel='linear'),\n",
    "          #svm.LinearSVC(),\n",
    "          svm.SVC(kernel='rbf'),\n",
    "          svm.SVC(kernel='poly', degree=3))\n",
    "models = (clf.fit(X_train, y_train) for clf in models)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "def Visualize():\n",
    "    roc_auc = np.zeros((3,1))\n",
    "    score = np.zeros((3,1))\n",
    "    fignum=0\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    for clf in models:\n",
    "\n",
    "        plt.figure(fignum, figsize=(15, 15))\n",
    "        plt.clf()\n",
    "\n",
    "        plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "                    facecolors='none', zorder=10, edgecolors='k')\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], zorder=10, cmap=plt.cm.Paired,\n",
    "                   edgecolors='k')\n",
    "\n",
    "        mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "        plt.axis('tight')\n",
    "        x_min = np.min(X_train[:,0])\n",
    "        x_max = np.max(X_train[:,0])\n",
    "        y_min = np.min(X_train[:,1])\n",
    "        y_max = np.max(X_train[:,1])\n",
    "\n",
    "        XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j] # Make a grid\n",
    "        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]) # Calculate which color for the map using the clf \n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(XX.shape)\n",
    "        plt.figure(fignum, figsize=(4, 3))\n",
    "        #plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "        plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.jet)\n",
    "        plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "                    levels=[-.5, 0, .5])\n",
    "\n",
    "        plt.xlim(x_min, x_max)\n",
    "        plt.ylim(y_min, y_max)\n",
    "\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        y_score = clf.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "        roc_auc[fignum] = auc(fpr, tpr)\n",
    "        \n",
    "        score[fignum]=clf.score(X_test,y_test)\n",
    "        \n",
    "        print(\"Support vectors:\")\n",
    "        print(clf.support_vectors_[:].shape)\n",
    "\n",
    "        if fignum==3:\n",
    "            print('score= ',score)\n",
    "            print('roc_auc= ',roc_auc)\n",
    "        fignum = fignum + 1\n",
    "\n",
    "#Visualize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 . Pick the RBF kernel and optimize the two most important hyperparameters (the $C$ parameter and the kernel width $\\gamma$). \n",
    "\n",
    "- First, optimize manually using 3 values for each (a very small, default, and very large value). For each of the 9 combinations, create the same RBF plot as before, report the number of support vectors, and the AUC performance. Explain the performance results. When are you over/underfitting?\n",
    "- Next, optimize the hyperparameters using a grid search and 10-fold cross validation. Show a heatmap of the results and report the optimal hyperparameter values.\n",
    "    - Hint: values for C and $\\gamma$ are typically in [$2^{-15}..2^{15}$] on a log scale. Use at least 10 values for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3975, 2)\n",
      "(5300,)\n"
     ]
    }
   ],
   "source": [
    "#C = 1.0  # SVM regularization parameter\n",
    "models = (svm.SVC(kernel='rbf',C=2e-15,gamma=2e-15),svm.SVC(kernel='rbf',C=2e-15),\n",
    "          svm.SVC(kernel='rbf',C=2e-15,gamma=2e15),\n",
    "          svm.SVC(kernel='rbf',gamma=2e-15),svm.SVC(kernel='rbf'),\n",
    "          svm.SVC(kernel='rbf',gamma=2e15),\n",
    "          svm.SVC(kernel='rbf',C=2e15,gamma=2e-15),svm.SVC(kernel='rbf',C=2e15),\n",
    "          svm.SVC(kernel='rbf',C=2e15,gamma=2e15),)\n",
    "models = (clf.fit(X_train, y_train) for clf in models)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "def Visualize():\n",
    "    roc_auc = np.zeros((9,1))\n",
    "    score = np.zeros((9,1))\n",
    "    fignum=0\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    \n",
    "    for clf in models:\n",
    "        plt.figure(fignum, figsize=(15, 15))\n",
    "        plt.clf()\n",
    "\n",
    "        plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "                    facecolors='none', zorder=10, edgecolors='k')\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], zorder=10, cmap=plt.cm.Paired,\n",
    "                   edgecolors='k')\n",
    "\n",
    "        mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "        plt.axis('tight')\n",
    "        x_min = np.min(X_train[:,0])\n",
    "        x_max = np.max(X_train[:,0])\n",
    "        y_min = np.min(X_train[:,1])\n",
    "        y_max = np.max(X_train[:,1])\n",
    "\n",
    "        XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j] # Make a grid\n",
    "        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]) # Calculate which color for the map using the clf \n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(XX.shape)\n",
    "        plt.figure(fignum, figsize=(4, 3))\n",
    "        #plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "        plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.jet)\n",
    "        plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "                    levels=[-.5, 0, .5])\n",
    "\n",
    "        plt.xlim(x_min, x_max)\n",
    "        plt.ylim(y_min, y_max)\n",
    "\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        y_score = clf.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "        roc_auc[fignum] = auc(fpr, tpr)\n",
    "        \n",
    "        score[fignum]=clf.score(X_test,y_test)\n",
    "        \n",
    "        print(\"Support vectors:\")\n",
    "        print(clf.support_vectors_[:].shape)\n",
    "\n",
    "        if fignum==9:\n",
    "            print('score= ',score)\n",
    "            print('roc_auc= ',roc_auc)\n",
    "        fignum = fignum + 1\n",
    "\n",
    "#Visualize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV \n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "CA=np.logspace(-15,15,20)\n",
    "gammaA=np.logspace(-15,15,20)\n",
    "p_grid = {\"C\": CA,\n",
    "      \"gamma\": gammaA}\n",
    "clf=svm.SVC(kernel='rbf')\n",
    "grid = GridSearchCV(clf, param_grid=p_grid, cv=10, n_jobs=1)    \n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "print(\"Best params:\\n{}\\n\".format(grid.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "scores = np.array(results.mean_test_score).reshape(len(CA),len(gammaA))\n",
    "    \n",
    "# plot the mean cross-validation scores\n",
    "plt.figure(1, figsize=(15, 15))\n",
    "print(p_grid['gamma'])\n",
    "print(np.array(map(str,p_grid['gamma'])))\n",
    "str1=np.char.mod('%.3e',CA)\n",
    "str2=np.char.mod('%.3e',gammaA)\n",
    "mglearn.tools.heatmap(scores, ylabel='C', yticklabels=str1,\n",
    "                  xlabel='gamma', xticklabels=str2, cmap=\"viridis\",fmt=\"%.3f\")\n",
    "        #matplotlib.ticker.LogFormatterExponent(base=10.0, labelOnlyBase=False, minor_thresholds=None, linthresh=None));\n",
    "#plt.ticklabel_format(style='sci')\n",
    "ax = plt.gca\n",
    "#ax.XData = str(np.array(gammaA),fmt=\"%.3f\")\n",
    "plt.title(\"ElasticNet score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building Kernels (4 points (0.5+0.5+1+2))\n",
    "\n",
    "Consider the artificial dataset given below. It represents a sine wave with added noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit an SVM Regressor with the default RBF kernel, and plot the predictions on all data points in [0, 40]. \n",
    "\n",
    "    - Does it fit the data well? Does it extrapolate well (in the range [30,40])? Explain your findings. \n",
    "    - Can you get better results by tweaking the kernel or the other SVM parameters?\n",
    "    \n",
    "2. Implement your own linear kernel. This is a function that takes 2 vectors (arrays) and returns the dot product:\n",
    "\n",
    "      $$k(\\mathbf{x}_i,\\,\\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j$$\n",
    "    - Build an SVM regressor using that kernel by passing your kernel function as the `kernel` hyperparameter. \n",
    "    - Fit it on the sine data and plot the predictions on all data points. Interpret the results.\n",
    "    \n",
    "3. Since this data is periodic, it makes sense to implement a periodic kernel instead. \n",
    "\n",
    "    - This is the Exponential Sine Squared kernel, with length scale $\\Gamma$ and periodicity $P$:\n",
    "  $$k(\\mathbf{x}_i,\\,\\mathbf{x}_j) = \\exp \\left( -\\Gamma\\,\\sin^2\\left[\\frac{\\pi}{P}\\,\\left|\\left|x_i-x_j\\right|\\right|\\right]\\right)$$\n",
    "    - Implement it, using the defaults $\\Gamma=1$, periodicity $P=1$, and Euclidean distance.\n",
    "    - Train an SVM regressor with it, fit in on the same data and plot the result. Interpret the outcome. \n",
    "    - Think about what $\\Gamma$ and $P$ represent. Can you improve the fit by manually adjusting them? Explain your findings.\n",
    "    - Optimize $\\Gamma$ and periodicity $P$ (using `true_y` as the ground truth). Use a grid search or random search, $\\Gamma \\in [0,1]$, $P \\in [1,100]$, try at least 5 values for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "rng = np.random.RandomState(0) # Random seed, for reproducibility \n",
    "X = 30 * rng.rand(200, 1)\n",
    "y = np.sin(X).ravel() \n",
    "y[::2] += rng.normal(scale = 1.0, size = X.shape[0] // 2) # adds noise\n",
    "\n",
    "X_plot = np.linspace(0, 40, 10000)[:, None] # A larger range to evaluate on\n",
    "true_y = np.sin(X_plot) # and the 'true' target function\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X, y, c='k', label='Data')\n",
    "plt.plot(X_plot, true_y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 . We now make the problem a bit more challenging by adding an upward trend:\n",
    "\n",
    "- Fit the same SVM using the optimal parameters from the previous subtask and plot the results. Do they still work? Explain what you see.\n",
    "- Fit a Gaussian process (GP) using the kernels given below. First use the singular ExpSineSquared kernel (the implementation provided by sklearn this time), then build a new kernel consisting of the 3 components given below. Use both to predict all points for the \"rising noisy sine\" data and plot the results as usual. Interpret the results.\n",
    "    - For the GP, it may help to use `normalize_y=True` since the y-values are not around 0. Setting `alpha=0.1` may help with possible numerical issues, otherwise keep it at 0.\n",
    "- Also plot the _uncertainty interval_ around the predictions. You can ask the GP to return the standard deviation during prediction with the `return_std=True` hyperparameter. Plot a band 2 standard deviations above and below the prediction. You can use MatPlotLib's `fill_between` as shown in class.\n",
    "    - You can combine the 3 models in one plot for easy comparison.\n",
    "- We've provided reasonable values for the kernel hyperparameters above. Can you optimize them further to get an even better fit? Think about what the hyperparameters do and optimize the ones you think are most worth tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data with added trend\n",
    "rng = np.random.RandomState(0)\n",
    "X = 30 * rng.rand(200, 1)\n",
    "y = X.ravel()/2 + np.sin(X).ravel()\n",
    "y[::2] += rng.normal(scale = 1.0, size = X.shape[0] // 2)  # add noise\n",
    "\n",
    "X_plot = np.linspace(0, 40, 10000)[:, None]\n",
    "true_y = X_plot/2 + np.sin(X_plot)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X, y, c='k', label='Data')\n",
    "plt.plot(X_plot, true_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared, RBF\n",
    "\n",
    "# Replace `length_scale` and `periodicity` with the values found in the previous part.\n",
    "kernel_simple = ExpSineSquared(length_scale=1, periodicity=1) # periodic component\n",
    "\n",
    "k1 = 4300 * RBF(length_scale=70.0)  # long term smooth rising trend\n",
    "k2 = 6 * RBF(length_scale=90.0) * ExpSineSquared(length_scale=1.3, periodicity=1.0)  # periodic component\n",
    "k3 = 0.03 * RBF(length_scale=0.134) + WhiteKernel(noise_level=0.035)  # noise terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian updates (3 points (2+1))\n",
    "\n",
    "We consider real data about solar radiation measured by a weather balloon:\n",
    "https://www.openml.org/d/512. We'll use only the raw data (at least the first 1000 points) and try to learn the (very noisy) trend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Train a Gaussian process on an increasing amount of samples of the training data. Use a simple RBF kernel:\n",
    "`RBF(10, (1e-2, 1e2))`\n",
    "\n",
    " - Start with 10 _random_ samples and plot the predictions (both the mean and the uncertainty interval) for both training and test data, as shown in class. Also compute $R^2$ on the training data.\n",
    " - Repeat and 10 more points, retrain and redraw. Do this a couple of times and interpret/explain what you see. \n",
    " \n",
    "2. Train the Gaussian on the full training set.\n",
    "\n",
    " - Plot the predictions (including the uncertainty interval) on the full dataset. Evaluate on the test set using $R^2$ \n",
    " - Interpret the results. Is the kernel right? Is the GP under/overfitting?\n",
    " - Try to improve the results by tuning the kernel. Do this either manually or using a small grid/random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Get the data\n",
    "balloon_data = oml.datasets.get_dataset(512) # Download Balloon data\n",
    "X, y = balloon_data.get_data(target=balloon_data.default_target_attribute);\n",
    "\n",
    "train = X[:1000]\n",
    "test = X[1000:1500]\n",
    "X_train = np.array(list(range(1000)))[np.newaxis].T\n",
    "y_train = X[:1000].reshape(-1, 1)\n",
    "X_test = np.array(list(range(1000,1250)))[np.newaxis].T\n",
    "y_test = X[1000:1500].reshape(-1, 1)\n",
    "X_all = np.array(list(range(1500)))[np.newaxis].T\n",
    "\n",
    "pd.Series(X[:1500,0]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A data mining challenge (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to use everything you have learned to build the best model for a given classification task. We will use two tasks hosted on OpenML, so you will all receive the same cross-validation splits, and your model will be evaluated on the server. The goal is to reasonably select algorithms and hyperparameter settings to obtain the best model. You can also do model selection, pipeline building, and parameter optimization as you have done before. Skeleton code is provided in the OpenML tutorial. You need to optimize the AUROC score (calculated using 10-fold cross0-validation).\n",
    "\n",
    "- Challenge 1: Detects accents in speech data.\n",
    "\n",
    "    - The OpenML Task ID is 167132: https://www.openml.org/t/167132\n",
    "    - The dataset description can be found here: https://www.openml.org/d/40910\n",
    "    - Leaderboard: https://www.openml.org/t/167132#!people\n",
    "    \n",
    "- Challenge 2: Image recognition (CIFAR-10 subsample).\n",
    "\n",
    "    - The OpenML Task ID is 167133: https://www.openml.org/t/167133\n",
    "    - The dataset description can be found here: https://www.openml.org/d/40926\n",
    "    - Leaderboard: https://www.openml.org/t/167133#!people\n",
    "    - Note that this is a high-dimensional dataset (and not so small). Think carefully about how to run experiments in the  time available.\n",
    "    \n",
    "- You are able to see the solutions of others (by clicking in the timeline or run list), so you can learn from prior experiments (what seems to work, how long does it take to train certain models, ...). Resubmission of the exact same solution is not possible.\n",
    "- You can share one account (one API key) per team. In case you use two, we take the one that performs best.\n",
    "- Document the different experiments that you ran in this notebook (running them can of course be done outside of the notebook). For each experiment, provide a description of how and why you chose the algorithms and parameters that you submitted. Reason about which experiments to try, don't just do an immense random search.\n",
    "- Points are rewarded as follows (independently for each task):\n",
    "\n",
    "    - 1 point for the breadth of experiments you ran (algorithms, pipelines, hyperparameter settings)\n",
    "    - 1 point for reasoning/insight and interpretation of the results\n",
    "    - 1 (bonus) point for every team who has uploaded the best solution thus far **on AUC** (who reaches the top of the leaderboard at any moment during the assignment)\n",
    "        - Exception: simply repeating top models with nearly identical hyperparameters. This will be checked on the timeline.\n",
    "        - Note: On the leaderboard page, the 'frontier' line is drawn, and your top ranking is also shown in the leaderboard.\n",
    "        \n",
    "Note: Report the AUC scores of your best models in your report as well. In case of issues with OpenML we will use the experiments and scores mentioned your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
