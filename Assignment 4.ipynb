{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Mining: Assignment 4\n",
    "\n",
    "Please complete all assignments in this notebook. You should submit this notebook, as well as a PDF version (See File > Download as).\n",
    "\n",
    "**Deadline:** Thursday, April 12, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please fill in your names here\n",
    "NAME_STUDENT_1 = \"\"\n",
    "NAME_STUDENT_2 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *\n",
    "plt.rcParams['savefig.dpi'] = 100 # This controls the size of your figures\n",
    "# Comment out and restart notebook if you only want the last output of each cell.\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Backpropagation (6 points)\n",
    "\n",
    "Figure 1 illustrates a simple neural network model.\n",
    "\n",
    "![Figure 1](images/a4_network.png)\n",
    "\n",
    "It has single input $x$, and three layers with respectively one, two, and one neurons. The activation function of the neurons is ReLU. \n",
    "\n",
    "The parameters $w_1$, $w_2$, $w_3$, $w_4$, and $w_5$ (no biases) are initialized to the following values $w_1 = 2, w_2 = 1$, $w_3 = 2$, $w_4 = 4$, and $w_5 = 1$. Implement a single update step of the gradient descent algorithm by hand. Run the update state for the data point $(x=2, y=3)$:\n",
    "\n",
    "The goal is to model the relationship between two continuous variables. The learning rate is set to $0.1$\n",
    "\n",
    "Provide the solution in the following format:\n",
    "\n",
    "- A choice for a loss function \n",
    "- Compute graph for training the neural network\n",
    "- Partial derivative expression for each of the parameters in the model\n",
    "- The update expression for each of the parameters for each of the data-points\n",
    "- The final value of all five parameters after the single step in the gradient descent algorithm\n",
    "\n",
    "The Python code for simple computational graph nodes, as seen in the tutorial session, is provided in the cell below (run the cell to load the code, and again to run the code). Extend the nodes so they can be used to implement the network described above. Implement the network with the same initial weights and the correct learning rate, and verify your hand-made calculations. Add comments to your code or provide a separate description to explain the changes you have made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load basic_graph.py\n",
    "'''\n",
    "Implementations of nodes for a computation graph. Each node\n",
    "has a forward pass and a backward pass function, allowing\n",
    "for the evaluation and backpropagation of data.\n",
    "'''\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        ''' Feed-forward the result '''\n",
    "        raise NotImplementedError(\"Missing forward-propagation method.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, d):\n",
    "        ''' Back-propagate the error\n",
    "            d is the delta of the subsequent node in the network '''\n",
    "        raise NotImplementedError(\"Missing back-propagation method.\")\n",
    "\n",
    "\n",
    "class ConstantNode(Node):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.output = value\n",
    "\n",
    "    def forward(self):\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        pass\n",
    "\n",
    "\n",
    "class VariableNode(Node):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.output = value\n",
    "\n",
    "    def forward(self):\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.output -= 0.1 * d # Gradient Descent\n",
    "\n",
    "\n",
    "class AdditionNode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = sum([i.forward() for i in self.inputs])\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        for i in self.inputs:\n",
    "            i.backward(d)\n",
    "\n",
    "\n",
    "class MultiplicationNode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = self.inputs[0].forward() * self.inputs[1].forward()\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.inputs[0].backward(d * self.inputs[1].output)\n",
    "        self.inputs[1].backward(d * self.inputs[0].output)\n",
    "\n",
    "\n",
    "class MSENode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = 0.5 * (\n",
    "            self.inputs[0].forward() - self.inputs[1].forward())**2\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.inputs[0].backward(d * (self.inputs[0].output - self.inputs[1].output))\n",
    "        self.inputs[1].backward(d * (self.inputs[1].output - self.inputs[0].output))\n",
    "\n",
    "\n",
    "class SigmoidNode(Node):\n",
    "\n",
    "    def forward(self):\n",
    "        self.output = 1.0 / (1.0 + math.exp(-self.inputs[0].forward()))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.inputs[0].backward(d * self.output * (1.0 - self.output))\n",
    "\n",
    "class ReLUNode(object):\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError(\"Forward pass for ReLU activation node has not been implemented yet.\")\n",
    "\n",
    "    def backward(self, d):\n",
    "        raise NotImplementedError(\"Backward pass for ReLU activation node has not been implemented yet.\")\n",
    "\n",
    "class TanhNode(object):\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError(\"Forward pass for tanh activation node has not been implemented yet.\")\n",
    "\n",
    "    def backward(self, d):\n",
    "        raise NotImplementedError(\"Backward pass for tanh activation node has not been implemented yet.\")\n",
    "\n",
    "# Example graph as shown in MLP lecture slides\n",
    "class SampleGraph(object):\n",
    "\n",
    "    def __init__(self, x, y, w, b):\n",
    "        ''' x: input\n",
    "            y: expected output\n",
    "            w: initial weight\n",
    "            b: initial bias '''\n",
    "        self.w = VariableNode(w)\n",
    "        self.b = VariableNode(b)\n",
    "        self.graph = MSENode([\n",
    "            AdditionNode([\n",
    "                MultiplicationNode([\n",
    "                    ConstantNode(x),\n",
    "                    self.w\n",
    "                ]),\n",
    "                MultiplicationNode([\n",
    "                    self.b,\n",
    "                    ConstantNode(1)\n",
    "                ])\n",
    "            ]),\n",
    "            ConstantNode(y)\n",
    "        ])\n",
    "\n",
    "    def forward(self):\n",
    "        return self.graph.forward()\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.graph.backward(d)\n",
    "\n",
    "\n",
    "class Neuron(Node):\n",
    "\n",
    "    def __init__(self, inputs, weights, activation):\n",
    "        ''' weights: list of initial weights, same length as inputs '''\n",
    "        self.inputs = inputs\n",
    "        # Initialize a weight for each input\n",
    "        self.weights = [VariableNode(weight) for weight in weights]\n",
    "        # Neurons normally have a bias, ignore for this assignment\n",
    "        #self.bias = VariableNode(bias, \"b\")\n",
    "\n",
    "        # Multiplication node for each pair of inputs and weights\n",
    "        mults = [MultiplicationNode([i, w]) for i, w, in zip(self.inputs, self.weights)]\n",
    "        # Neurons normally have a bias, ignore for this assignment\n",
    "        #mults.append(MultiplicationNode([self.bias, ConstantNode(1)]))\n",
    "\n",
    "        # Sum all multiplication results\n",
    "        added = AdditionNode(mults)\n",
    "\n",
    "        # Apply activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.graph = SigmoidNode([added])\n",
    "        elif activation == 'relu':\n",
    "            self.graph = ReLUNode([added])\n",
    "        elif activation == 'tanh':\n",
    "            self.graph = TanhNode([added])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function.\")\n",
    "\n",
    "    def forward(self):\n",
    "        return self.graph.forward()\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.graph.backward(d)\n",
    "\n",
    "    def set_weights(self, new_weights):\n",
    "        for i in len(new_weights):\n",
    "            self.weights[i].output = new_weights[i]\n",
    "\n",
    "    def get_weights(self):\n",
    "        return [weight.output for weight in self.weights]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Loaded simple graph nodes\")\n",
    "\n",
    "    # Example network\n",
    "    #sg = SampleGraph(2, 2, 2, 1)\n",
    "    #prediction = sg.forward()\n",
    "    #print(\"Initial prediction is\", prediction)\n",
    "    #sg.backward(1)\n",
    "    #print(\"w has new value\", sg.w.output)\n",
    "    #print(\"b has new value\", sg.b.output)\n",
    "\n",
    "    # Run your network here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Deep Models (3 points)\n",
    "\n",
    "The model in the example code below performs poorly as its depth increases. Train this model on the MNIST digit detection task. \n",
    "\n",
    "Examine its training performance by gradually increasing its depth:\n",
    "- Set the depth to 1 hidden layer\n",
    "- Set the depth to 2 hidden layers\n",
    "- Set the depth to 3 hidden layers\n",
    "\n",
    "Modify the model such that you improve its performance when its depth increases. Train the new model again for the different depths:\n",
    "- Set the depth to 1 hidden layer\n",
    "- Set the depth to 2 hidden layers\n",
    "- Set the depth to 3 hidden layers\n",
    "\n",
    "Submit an explanation for the limitation of the original model. Explain your modification. \n",
    "Submit your code and 6 plots (can be overlaid) for the training performance of both models with different depths. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# (You don't need to change this part of the code)\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 90\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# (You don't need to change this part of the code)\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this parameter to change the depth of the model\n",
    "number_hidden_layers = 1  # Number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,), activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "while number_hidden_layers > 1:\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    number_hidden_layers -= 1\n",
    "\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_14 to have shape (10,) but got array with shape (90,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-626d76c5b483>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m history = model.fit(X_train, Y_train,\n\u001b[0;32m      3\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                     verbose=1, validation_data=(X_test, Y_test))\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bramv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\bramv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bramv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32mc:\\users\\bramv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    121\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected activation_14 to have shape (10,) but got array with shape (90,)"
     ]
    }
   ],
   "source": [
    "# Training (You don't need to change this part of the code)\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Calculator (6 points)\n",
    "\n",
    "During the lectures you have seen a CNN model that can be successfully trained to classify the MNIST images. You have also seen how a RNN model that can be trained to implement addition of two numbers. You now need to build a model that is a combination of convolutional layers and recurrent cells. \n",
    "\n",
    "Using the KERAS library, design and train a model that produces a sum of a sequence of MNIST images. More specifically, the model should input a sequence of 10 images and compute the cumulative sum of the digits represented by the images.\n",
    "\n",
    "For example:\n",
    "\n",
    "Input 1: ![294](images/a3ex1.png)\n",
    "\n",
    "Output 1: 46\n",
    "\n",
    "Input 2: ![61](images/a3ex2.png)\n",
    "\n",
    "Output 2: 43\n",
    "\n",
    "Your solutions should include:\n",
    "- Python code that formats the MNIST dataset such that it can be used for traning and testing your model\n",
    "- Implementation in keras of your model (for training and testing)\n",
    "- Performance on the model on test data\n",
    "- Justification (in text) of your decisions for the model architecture (type of layers, activation functions, loss function, regularization and training hyperparameters)\n",
    "\n",
    "Note: Use the 60000/10000 train/test split of the MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Provide your solution here\n",
    "\n",
    "# Imports\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, Dense, TimeDistributed, Activation\n",
    "#from keras import layers \n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n",
    "import numpy as np\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 12\n",
    "num_classes = 90\n",
    "num_images = 10\n",
    "epochs = 12\n",
    "\n",
    "TRAINING_SIZE = 6000\n",
    "TESTING_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "\n",
    "class Round(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Round, self).__init__(**kwargs)\n",
    "\n",
    "    def get_output(self, train=False):\n",
    "        X = self.get_input(train)\n",
    "        return K.round(X)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"name\": self.__class__.__name__}\n",
    "        base_config = super(Round, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2af4f81ad30>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ad0919af98>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,' 5')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2af486a1f28>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ad4e2b58d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,' 0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2ad4e2b59e8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ad4e2ef7b8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,' 4')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2ad4e2efb38>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ad4e32d7b8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,' 1')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2ad4e32d898>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ad4e29c0f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,' 9')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2ad4e2929e8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ad4e394ef0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,' 2')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2ad4e3aa0f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ad4e3cfc18>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,' 1')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2ad4e3cfc88>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ad4e409a90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,' 3')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2ad4e409cf8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ad4e3a1c50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,' 1')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2ad4e36ba20>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ad4e47b240>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,' 4')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) train samples\n",
      "(10000, 28, 28) test samples\n",
      "(6000, 10, 784) train samples\n",
      "(1000, 10, 784) test samples\n"
     ]
    },
    {
     "data": {
      "application/pdf": "JVBERi0xLjQKJazcIKu6CjEgMCBvYmoKPDwgL1BhZ2VzIDIgMCBSIC9UeXBlIC9DYXRhbG9nID4+CmVuZG9iago4IDAgb2JqCjw8IC9FeHRHU3RhdGUgNCAwIFIgL0ZvbnQgMyAwIFIgL1BhdHRlcm4gNSAwIFIKL1Byb2NTZXQgWyAvUERGIC9UZXh0IC9JbWFnZUIgL0ltYWdlQyAvSW1hZ2VJIF0gL1NoYWRpbmcgNiAwIFIKL1hPYmplY3QgNyAwIFIgPj4KZW5kb2JqCjEwIDAgb2JqCjw8IC9Bbm5vdHMgWyBdIC9Db250ZW50cyA5IDAgUgovR3JvdXAgPDwgL0NTIC9EZXZpY2VSR0IgL1MgL1RyYW5zcGFyZW5jeSAvVHlwZSAvR3JvdXAgPj4KL01lZGlhQm94IFsgMCAwIDQyOC4yMTg3NSA3OS4yOTg1Njk5MTUzIF0gL1BhcmVudCAyIDAgUiAvUmVzb3VyY2VzIDggMCBSCi9UeXBlIC9QYWdlID4+CmVuZG9iago5IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMTEgMCBSID4+CnN0cmVhbQp4nLXWzW7TQBAA4Ps+xRzhwGb+Z/fYqiXiWBSJA+KUhkJEi9pK8PpsTJCcykulWD7E9m6smfnkHa8J9ml1QXD3DAj79vsNn+FLO98CwRpWV7tf37e7j+tL2D4nbPP3SblkphLWRj9Go6iZa7FokzgefEvpIT0Ce67DfSy5xHAlkgnJq1YyeTF62sEneIDVBR9qo1Yb/a0tHWpDWLeQBIeKaCr09h5Eh7/fDed2aFOrDwRXP+Em3cDjv9AId+PwcAx/mE+XG1i9JyCGzdckkUMFRauFg3OuIeZ1qHZzm96AvYXNHq43Q3T37IpRpNQi55rTi6JG5l78HpzPh0dkdUMvxaxOwXEMJ/QszoUiqvsC8m6CHl3OpxNFbrEJOUx5yq4ndvWMRYsNsCXsvQQ9u86wW2srrCTuxjZlpxN7sVyOi0R4CXsvQc9uM+y1tZe0p49mWKbsdWxntmxcKg2sBezdBD27n29naf1lwqiqdXLN84ndLbNhNTm8ipaw9xL07DHDHq2/wliKaLy+5gW13dA2uuBSaAF7N0HPXmbsb2Q5WrJgVptc83JiV81eDZmouC5h7yXo2esMu7X+oiropEqvP/eiWY8vB40l7L0E3e8anIGvrcGURBGV9f+bXPoD0YAi/QplbmRzdHJlYW0KZW5kb2JqCjExIDAgb2JqCjQ4MgplbmRvYmoKMjYgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAyNDcgPj4Kc3RyZWFtCnicTVG7bUQxDOvfFFzgAOtreZ4LUl32b0PJCJDCIKEvKaclFvbGSwzhB1sPvuSRVUN/Hj8x7DMsPcnk1D/muclUFL4VqpuYUBdi4f1oBLwWdC8iK8oH349lDHPO9+CjEJdgJjRgrG9JJhfVvDNkwomhjsNBm1QYd00ULK4VzTPI7VY3sjqzIGx4JRPixgBEBNkXkM1go4yxlZDFch6oCpIFWmDX6RtRi4IrlNYJdKLWxLrM4Kvn9nY3Qy/y4Ki6eH0M60uwwuileyx8rkIfzPRMO3dJI73wphMRZg8FUpmdkZU6PWJ9t0D/n2Ur+PvJz/P9CxUoXCoKZW5kc3RyZWFtCmVuZG9iagoyNyAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDkwID4+CnN0cmVhbQp4nE2NQRLAIAgD77wiT1BE0P90etL/X6vUDr3ATgKJFkWC9DVqSzDuuDIVa1ApmJSXwFUwXAva7qLK/jJJTJ2G03u3A4Oy8XGD0kn79nF6AKv9egbdD9IcIlgKZW5kc3RyZWFtCmVuZG9iagoyOCAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDMyMCA+PgpzdHJlYW0KeJw1UbtxxTAM6zUFF/Cd+JU0j3Ovytu/DUA7FWEaBECqvGRKuVzqklWywuRHh+oUTfk+YKb8DvWQ4+ge2SG6U9aWexgIy8Q8pY5YTZZ7uAWBLwxNibmF8/cI6CsGozATgbrF3z9AsyQwaXDwU5BrrVpiiQ48LBZYsyvMrRopVMhVfDs2uQcFcnGz0KccmhS33ILwZYhkR2qxr8tlKfK79QkYhBXmiE8UiYXngQ5mIvEnA2J79tliV1cvqhEZ1kmHB1IE0mxuEjA0RbLqgxvYV8c1P09H2cHJQb+Kwfg2OJkvSXlfBaEQjxf+Ds/ZyLGSQyQU8n21wIgjbIARoU/tIxBlIDRF9+6ZUj4mVYrvAEYhHH2qVzK8F5HZaobN/xld2SoKBlVZH59GcCaDSTjzZKMK01K107/73OPzB2NjeoAKZW5kc3RyZWFtCmVuZG9iagoyOSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDgwID4+CnN0cmVhbQp4nEWMuw3AMAhEe6ZgBH4mZp8olbN/GyBK3HBPunu4OhIyU95hhocEngwshlPxBpmjYDW4RlKNneyjsG5fdYHmelOr9fcHKk92dnE9zcsZ9AplbmRzdHJlYW0KZW5kb2JqCjMwIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMTcgPj4Kc3RyZWFtCnicMza0UDCAwxRDLgAalALsCmVuZHN0cmVhbQplbmRvYmoKMzEgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAzMzggPj4Kc3RyZWFtCnicNVI5rt1ADOt9Cl0ggHbNnOcFqX7u34aUXwpDtFaKmo4WlWn5ZSFVLZMuv+1JbYkb8vfJCokTklcl2qUMkVD5PIVUv2fLvL7WnBEgS5UKk5OSxyUL/gyX3i4c52NrP48jdz16YFWMhBIByxQTo2tZOrvDmo38PKYBP+IRcq5YtxxjFUgNunHaFe9D83nIGiBmmJaKCl1WiRZ+QfGgR61991hUWCDR7RxJcIyNUJGAdoHaSAw5sxa7qC/6WZSYCXTtiyLuosASScycYl06+g8+dCyovzbjy6+OSvpIK2tM2nejSWnMIpOul0VvN299PbhA8y7Kf17NIEFT1ihpfNCqnWMomhllhXccmgw0xxyHzBM8hzMSlPR9KH5fSya6KJE/Dg2hf18eo4ycBm8Bc9GftooDF/HZYa8cYIXSxZrkfUAqE3pg+v/X+Hn+/AMctoBUCmVuZHN0cmVhbQplbmRvYmoKMzIgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAyNDggPj4Kc3RyZWFtCnicLVE5kgNBCMvnFXpCc9PvscuR9//pCsoBg4ZDIDotcVDGTxCWK97yyFW04e+ZGMF3waHfynUbFjkQFUjSGFRNqF28Hr0HdhxmAvOkNSyDGesDP2MKN3pxeEzG2e11GTUEe9drT2ZQMisXccnEBVN12MiZw0+mjAvtXM8NyLkR1mUYpJuVxoyEI00hUkih6iapM0GQBKOrUaONHMV+6csjnWFVI2oM+1xL29dzE84aNDsWqzw5pUdXnMvJxQsrB/28zcBFVBqrPBAScL/bQ/2c7OQ33tK5s8X0+F5zsrwwFVjx5rUbkE21+Dcv4vg94+v5/AOopVsWCmVuZHN0cmVhbQplbmRvYmoKMzMgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAyMTAgPj4Kc3RyZWFtCnicNVDLDUMxCLtnChaoFAKBZJ5WvXX/a23QO2ER/0JYyJQIeanJzinpSz46TA+2Lr+xIgutdSXsypognivvoZmysdHY4mBwGiZegBY3YOhpjRo1dOGCpi6VQoHFJfCZfHV76L5PGXhqGXJ2BBFDyWAJaroWTVi0PJ+QTgHi/37D7i3koZLzyp4b+Ruc7fA7s27hJ2p2ItFyFTLUszTHGAgTRR48eUWmcOKz1nfVNBLUZgtOlgGuTj+MDgBgIl5ZgOyuRDlL0o6ln2+8x/cPQABTtAplbmRzdHJlYW0KZW5kb2JqCjI0IDAgb2JqCjw8IC9CYXNlRm9udCAvRGVqYVZ1U2FucyAvQ2hhclByb2NzIDI1IDAgUgovRW5jb2RpbmcgPDwKL0RpZmZlcmVuY2VzIFsgMzIgL3NwYWNlIDQ4IC96ZXJvIC9vbmUgL3R3byAvdGhyZWUgL2ZvdXIgL2ZpdmUgNTcgL25pbmUgXQovVHlwZSAvRW5jb2RpbmcgPj4KL0ZpcnN0Q2hhciAwIC9Gb250QkJveCBbIC0xMDIxIC00NjMgMTc5NCAxMjMzIF0gL0ZvbnREZXNjcmlwdG9yIDIzIDAgUgovRm9udE1hdHJpeCBbIDAuMDAxIDAgMCAwLjAwMSAwIDAgXSAvTGFzdENoYXIgMjU1IC9OYW1lIC9EZWphVnVTYW5zCi9TdWJ0eXBlIC9UeXBlMyAvVHlwZSAvRm9udCAvV2lkdGhzIDIyIDAgUiA+PgplbmRvYmoKMjMgMCBvYmoKPDwgL0FzY2VudCA5MjkgL0NhcEhlaWdodCAwIC9EZXNjZW50IC0yMzYgL0ZsYWdzIDMyCi9Gb250QkJveCBbIC0xMDIxIC00NjMgMTc5NCAxMjMzIF0gL0ZvbnROYW1lIC9EZWphVnVTYW5zIC9JdGFsaWNBbmdsZSAwCi9NYXhXaWR0aCAxMzQyIC9TdGVtViAwIC9UeXBlIC9Gb250RGVzY3JpcHRvciAvWEhlaWdodCAwID4+CmVuZG9iagoyMiAwIG9iagpbIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwCjYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgMzE4IDQwMSA0NjAgODM4IDYzNgo5NTAgNzgwIDI3NSAzOTAgMzkwIDUwMCA4MzggMzE4IDM2MSAzMTggMzM3IDYzNiA2MzYgNjM2IDYzNiA2MzYgNjM2IDYzNiA2MzYKNjM2IDYzNiAzMzcgMzM3IDgzOCA4MzggODM4IDUzMSAxMDAwIDY4NCA2ODYgNjk4IDc3MCA2MzIgNTc1IDc3NSA3NTIgMjk1CjI5NSA2NTYgNTU3IDg2MyA3NDggNzg3IDYwMyA3ODcgNjk1IDYzNSA2MTEgNzMyIDY4NCA5ODkgNjg1IDYxMSA2ODUgMzkwIDMzNwozOTAgODM4IDUwMCA1MDAgNjEzIDYzNSA1NTAgNjM1IDYxNSAzNTIgNjM1IDYzNCAyNzggMjc4IDU3OSAyNzggOTc0IDYzNCA2MTIKNjM1IDYzNSA0MTEgNTIxIDM5MiA2MzQgNTkyIDgxOCA1OTIgNTkyIDUyNSA2MzYgMzM3IDYzNiA4MzggNjAwIDYzNiA2MDAgMzE4CjM1MiA1MTggMTAwMCA1MDAgNTAwIDUwMCAxMzQyIDYzNSA0MDAgMTA3MCA2MDAgNjg1IDYwMCA2MDAgMzE4IDMxOCA1MTggNTE4CjU5MCA1MDAgMTAwMCA1MDAgMTAwMCA1MjEgNDAwIDEwMjMgNjAwIDUyNSA2MTEgMzE4IDQwMSA2MzYgNjM2IDYzNiA2MzYgMzM3CjUwMCA1MDAgMTAwMCA0NzEgNjEyIDgzOCAzNjEgMTAwMCA1MDAgNTAwIDgzOCA0MDEgNDAxIDUwMCA2MzYgNjM2IDMxOCA1MDAKNDAxIDQ3MSA2MTIgOTY5IDk2OSA5NjkgNTMxIDY4NCA2ODQgNjg0IDY4NCA2ODQgNjg0IDk3NCA2OTggNjMyIDYzMiA2MzIgNjMyCjI5NSAyOTUgMjk1IDI5NSA3NzUgNzQ4IDc4NyA3ODcgNzg3IDc4NyA3ODcgODM4IDc4NyA3MzIgNzMyIDczMiA3MzIgNjExIDYwNQo2MzAgNjEzIDYxMyA2MTMgNjEzIDYxMyA2MTMgOTgyIDU1MCA2MTUgNjE1IDYxNSA2MTUgMjc4IDI3OCAyNzggMjc4IDYxMiA2MzQKNjEyIDYxMiA2MTIgNjEyIDYxMiA4MzggNjEyIDYzNCA2MzQgNjM0IDYzNCA1OTIgNjM1IDU5MiBdCmVuZG9iagoyNSAwIG9iago8PCAvZml2ZSAyNiAwIFIgL2ZvdXIgMjcgMCBSIC9uaW5lIDI4IDAgUiAvb25lIDI5IDAgUiAvc3BhY2UgMzAgMCBSCi90aHJlZSAzMSAwIFIgL3R3byAzMiAwIFIgL3plcm8gMzMgMCBSID4+CmVuZG9iagozIDAgb2JqCjw8IC9GMSAyNCAwIFIgPj4KZW5kb2JqCjQgMCBvYmoKPDwgL0ExIDw8IC9DQSAwIC9UeXBlIC9FeHRHU3RhdGUgL2NhIDEgPj4KL0EyIDw8IC9DQSAxIC9UeXBlIC9FeHRHU3RhdGUgL2NhIDEgPj4gPj4KZW5kb2JqCjUgMCBvYmoKPDwgPj4KZW5kb2JqCjYgMCBvYmoKPDwgPj4KZW5kb2JqCjcgMCBvYmoKPDwgL0kxIDEyIDAgUiAvSTEwIDIxIDAgUiAvSTIgMTMgMCBSIC9JMyAxNCAwIFIgL0k0IDE1IDAgUiAvSTUgMTYgMCBSCi9JNiAxNyAwIFIgL0k3IDE4IDAgUiAvSTggMTkgMCBSIC9JOSAyMCAwIFIgPj4KZW5kb2JqCjEyIDAgb2JqCjw8IC9CaXRzUGVyQ29tcG9uZW50IDggL0NvbG9yU3BhY2UgL0RldmljZVJHQgovRGVjb2RlUGFybXMgPDwgL0NvbG9ycyAzIC9Db2x1bW5zIDI4IC9QcmVkaWN0b3IgMTAgPj4gL0ZpbHRlciAvRmxhdGVEZWNvZGUKL0hlaWdodCAyOCAvTGVuZ3RoIDM0IDAgUiAvU3VidHlwZSAvSW1hZ2UgL1R5cGUgL1hPYmplY3QgL1dpZHRoIDI4ID4+CnN0cmVhbQpIie3Uu4ryQBgG4InaSEBDLDxBGgULxTatNpaSXEC8AW2s7BQUMWApWEQLKwsN2EVQSBMQLMRCLEQsIoilRoUQcXSLrEuQZdl/Z+Fv9i2HmYdvTh8Af3mG53n4zGKxqNVqlUqFIAgklKZpURS32y205Hw+V6tVHMeRaJIk+/3+er220oqiMAzjdDqRaK/Xm0wml8ullZ5MJizL2mw2JNrn8+Xz+c1mY6Wn02k6nUZyAQCRSKTRaOx2uw/3er1KkoTqAgDi8Xi5XB4Oh6Y7n89Rz8EawzAghIZhJBKJX+AIguA47na7QQjH4/EviLFYbDQamXtvNpvBYBBVZFn2eDyaYqFQ8Pv9qGIoFNI0TVXVTqeTy+UwDEMVcRyXZRlCmEqlUK2P1Ot1CKEsy99/QF/Nc7lcGIa53W4AQKvVut/vqAUyDNNut7PZrHk5s9nMbFTRaFQQhJ+IJEm+dCmzm0iSpOu6pmlfL3d8OkpRlMfjeRmkafp9jcPBcdzlcgEA7Pf7w+GwWq2+VSxFUeFwOJPJCILQ6/VeqlZVVRRFCOHpdFIU5Sf/1W63k88Ui0We5weDQSAQ6Ha7j8dD1/VSqfTP6F/+f94A5wss0wplbmRzdHJlYW0KZW5kb2JqCjM0IDAgb2JqCjQ1OAplbmRvYmoKMTMgMCBvYmoKPDwgL0JpdHNQZXJDb21wb25lbnQgOCAvQ29sb3JTcGFjZSAvRGV2aWNlUkdCCi9EZWNvZGVQYXJtcyA8PCAvQ29sb3JzIDMgL0NvbHVtbnMgMjggL1ByZWRpY3RvciAxMCA+PiAvRmlsdGVyIC9GbGF0ZURlY29kZQovSGVpZ2h0IDI4IC9MZW5ndGggMzUgMCBSIC9TdWJ0eXBlIC9JbWFnZSAvVHlwZSAvWE9iamVjdCAvV2lkdGggMjggPj4Kc3RyZWFtCkiJ7dSvy/JQFAfwyxuelTUxDGYTq8OBrGlbULA4s83qok00mAzqHzBQxiyW2QQtA20iFuMYU2HBKSiCcA57wngHLw/+mMKbnm/b7tmHw71nl5Df/J8wDFOv1wEAEQGg3W7H4/GPRJZlF4sFAAQoADiO0+l03kclSYK/2e12m80meDQMo9FovI/ebrdut8txXCwWkyRJ13XLshDRNE1N0wRBCIeWSiVEtCzr55Isy67rImK/33+n03s7yHGcqqoAMBqNQqOmad4roCiq2WwCgGVZoiiGQPf7PcMwD9xWq4WIk8mEpunnaD6fv16vAFCtVh9X+gPX6/VeanY+nyPibDZ7XOZ5HiIahvESKgiCP5iZTOZpp6+iFEXpug4Ap9NJFMVIJMKy7KcoISSRSKiqioiO46xWq2KxeA9VVfVVlBDCcZzruv4+jMdjmqZzuVytVotGowF6Pp9TqVQIlBDC87z/C/nnJssyItq2nUwmC4WC53nD4TCc6GcwGAQXSnBplcvl6XSKiJqm/fzkz1O0Uqlks9ntdhu8WS6Xl8vFH4z1ev1Op37S6bRt236nx+PRcRwAEATh6+vrfZT8e24AcDgcPuKC8DyvKAoiKooS+tB/80m+AV1LYnIKZW5kc3RyZWFtCmVuZG9iagozNSAwIG9iago0NzIKZW5kb2JqCjE0IDAgb2JqCjw8IC9CaXRzUGVyQ29tcG9uZW50IDggL0NvbG9yU3BhY2UgL0RldmljZVJHQgovRGVjb2RlUGFybXMgPDwgL0NvbG9ycyAzIC9Db2x1bW5zIDI4IC9QcmVkaWN0b3IgMTAgPj4gL0ZpbHRlciAvRmxhdGVEZWNvZGUKL0hlaWdodCAyOCAvTGVuZ3RoIDM2IDAgUiAvU3VidHlwZSAvSW1hZ2UgL1R5cGUgL1hPYmplY3QgL1dpZHRoIDI4ID4+CnN0cmVhbQpIie3UPY8pURgH8P9xNxMKMqLyUnpLJFQa7VAoNRqNyAQfQMInESqFUkEUWlRMoiJhKEwiCidRUDAKOXOLTXZtXLNWJrfaf3XO8zz55WTmzAC/+WGy2SxjrF6vPxowvYAmk0lCiM7AKygATdOMRwGMRiODUUIIY8xgVNO01WplMKqf19FUKvWo9eVmlMvlWCx2W+l0OovFYjwe3xbz+XytVluv116v9xu0Wq0WCoXD4bDZbN4r4XCYEHK9Xs/n83w+lyRpMpkMh0NK6Xa7tdvtHMf9E337WFksFkJIsVhstVrvlUgkQgiJx+Mej4fjuFwuJ4qiqqqSJJnNZpPJFAwGZVl+9BAAoNFoMMYEQdCZsdls6XS62+1eLhfGWCaT0RM/0Eql8s0cAKDZbDLGZFnmef6++/n2l8slgFAo9AxaKpUopX6/32q16qG9Xk9V1WdEAJRSRVEAuN1uPXQ6nVJKn0QBHI9HAIlEQg8FoPM530cURQA+n+++9ed2w/O8w+Fot9vPoKfTyel0RqPRfr+/3+9vW2+3m8FgMJvNnjwpY0xRFJfLpf9v/XEEQdjtdoFAwEj0N/8vfwH0Q6zRCmVuZHN0cmVhbQplbmRvYmoKMzYgMCBvYmoKNDQ1CmVuZG9iagoxNSAwIG9iago8PCAvQml0c1BlckNvbXBvbmVudCA4IC9Db2xvclNwYWNlIC9EZXZpY2VSR0IKL0RlY29kZVBhcm1zIDw8IC9Db2xvcnMgMyAvQ29sdW1ucyAyOCAvUHJlZGljdG9yIDEwID4+IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlCi9IZWlnaHQgMjggL0xlbmd0aCAzNyAwIFIgL1N1YnR5cGUgL0ltYWdlIC9UeXBlIC9YT2JqZWN0IC9XaWR0aCAyOCA+PgpzdHJlYW0KSInt1DGKg1AQBuBht7EQbASFFGplytQSNAEPkNbG2kMIXsDewsIL5ABpUjw8gm1SSJoQLSwNw+AWkirrshuHrfK3b973Bt4wAO98F13XhRBEdDqd/nr3Y+pguVw6jvNaQ5PonEyiXdddr1cAWCwWaZpKksTzYFEUwzAQEREZhsGDAgARISIiBkHAht5utxEty1KWZR50tVqdz+fR3e/3PCgAZFmGj9i2zYOapkmPFEXBgwLAbrfrug4RhRCaprG5URQhIhEdDgdFUX4u/vwl2rbtZrPRdd2yrPv9LoSY3ScAAKiqOo5t0zTb7ZYHBYA8z8cfOx6PbOh6vb5cLuN4hWHI5vq+37YtIlZVZRgG26JJkmRslohc130ueGWf1nXd9/3s3p4SxzERDcPgeR6//s4/5Qt/R65gCmVuZHN0cmVhbQplbmRvYmoKMzcgMCBvYmoKMzEyCmVuZG9iagoxNiAwIG9iago8PCAvQml0c1BlckNvbXBvbmVudCA4IC9Db2xvclNwYWNlIC9EZXZpY2VSR0IKL0RlY29kZVBhcm1zIDw8IC9Db2xvcnMgMyAvQ29sdW1ucyAyOCAvUHJlZGljdG9yIDEwID4+IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlCi9IZWlnaHQgMjggL0xlbmd0aCAzOCAwIFIgL1N1YnR5cGUgL0ltYWdlIC9UeXBlIC9YT2JqZWN0IC9XaWR0aCAyOCA+PgpzdHJlYW0KSIntlK2rKlEUxY9eBb8YLJ5oENNg1ikGk0mNGgz+EYpNEJtRm82mpklqGUHEYBVEnCCoyCgmBRnQvT03HPDB44HnDgOv3BUX+/z22ueLkP8uSZLa7TYAyLJsG3E8HgPAYDDw+/32QEulEgBommZbTEKIrusAoCiKbcRYLMYYQ8RQKPSx2CkITSaTr9eLMSZSLArdbreClT8TIgKAneOn02nx9qJQ0zSdTqfD4SgUChahiUTicrkg4mazaTabsizruj6dThljkUhEPPIfBYNBwzAAgG8i136/PxwOAGAYhhUopZSDwuEwpbRarc7n8+v1yns8n89MJmMl6el0AgBVVd9mNBpdLBa8Wb/ft/L2a7UaIpqmqaqq1+vlpsfjyWaz9/sdEev1+o+hhJBGo8HnbbVakiS9/fV6zfPmcjkrXP4tAcBoNHpzi8UiN3e7nchD+Fsul0tRlNvtBgCTySSVSrnd7kAgMBwO+d0ol8tWwhJC8vk8PzdE1DRtuVzypIjY6/X+ueTrI3S1Ws1ms8fj4fP54vE4pbRSqXS73fP53Ol0jsejxbC/+pW4vgGGbgWVCmVuZHN0cmVhbQplbmRvYmoKMzggMCBvYmoKNDMyCmVuZG9iagoxNyAwIG9iago8PCAvQml0c1BlckNvbXBvbmVudCA4IC9Db2xvclNwYWNlIC9EZXZpY2VSR0IKL0RlY29kZVBhcm1zIDw8IC9Db2xvcnMgMyAvQ29sdW1ucyAyOCAvUHJlZGljdG9yIDEwID4+IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlCi9IZWlnaHQgMjggL0xlbmd0aCAzOSAwIFIgL1N1YnR5cGUgL0ltYWdlIC9UeXBlIC9YT2JqZWN0IC9XaWR0aCAyOCA+PgpzdHJlYW0KSIntk7/raWEcx5/v9av8duoYlB/lDEZJBixnIxETNqvFxIaYmAwWFhaDOmVxUv4ClERZjlLUGcgiJNHz5A5PSe7lfq/vcLv1fU3PeXp/3ud9PufzAeCb/4KPh2eFQhEMBm0222azqdfr2+32crl89SXj8Rje0W63k8kkRVFCofB909VqBSE8Ho+j0YjjuJt7Op0WCARvmsbjcQjhfr+PRCIEQXi9XoZhDocDhDCbzer1+gd9NBplWZZlWYqinpoKBIJisQgh3O128Xhco9EAAOx2+2AwgBByHEfTNABAqVR6PJ5Go7Hb7fCnLBaLV2FFIlGhUMBSlmXVajUAgCTJ4XCIEDqdTvl8vtVqYUG326Vp2mq1Wq3WPzRBLBYHAgFclslkXC4XAIAgiMlkcv8bw+GwRCL5RFPv8jocDp7nIYSHw4HneYZh9vs9tlutVqlUSiwW/4XjDalUarfbWZZFCCGErtcrPvA8bzabn1U9Dv8zEomETCbD51AoZLPZer0eTdMIoXfC/kqtVsNNcLvdvxX8eFGs0+lyuZzT6dRqtff3/X7//USdTgcnms/nzWbTYDDge7/f/zrpK2Kx2P0AzWYzvFEURb1vqlKpqtXqdru9+XIcZzQav2SKsVgs1WoVrz9CaLlcTqdTCOF6vbZYLG+aYkwmU6VSud+ocrn8TPzZOcUoFAq5XO7z+UiSLJVK5/P5S0m/+cf8BMrPW1UKZW5kc3RyZWFtCmVuZG9iagozOSAwIG9iago1NDIKZW5kb2JqCjE4IDAgb2JqCjw8IC9CaXRzUGVyQ29tcG9uZW50IDggL0NvbG9yU3BhY2UgL0RldmljZVJHQgovRGVjb2RlUGFybXMgPDwgL0NvbG9ycyAzIC9Db2x1bW5zIDI4IC9QcmVkaWN0b3IgMTAgPj4gL0ZpbHRlciAvRmxhdGVEZWNvZGUKL0hlaWdodCAyOCAvTGVuZ3RoIDQwIDAgUiAvU3VidHlwZSAvSW1hZ2UgL1R5cGUgL1hPYmplY3QgL1dpZHRoIDI4ID4+CnN0cmVhbQpIie2SIQ6EMBBFp806NAZTgYFyB46D4wZwB7gGx8CiSDUCToBCMcwKsms2odNsk13BUxW/r7+dAtz8EKWUMWbfd0SM4/giKflSrXWappykg7QoCmbywZdmWSaEAAApHapcUZbl/qJpmusw98wkSb4u9gEivptqrf1IiQgREbHrOmuYO6jjOIgIAPq+t4Y9zfGPpFJKIcT5T604vymrASdU1zVTxyWKommazn9aVZVSyrrF3nTbtmVZzvUwDPM8e5Cu6zqOozXmJg2CgHNlN2kYhnmeO0lZtG1LRMYY18o33ngC3VpiqwplbmRzdHJlYW0KZW5kb2JqCjQwIDAgb2JqCjIzMAplbmRvYmoKMTkgMCBvYmoKPDwgL0JpdHNQZXJDb21wb25lbnQgOCAvQ29sb3JTcGFjZSAvRGV2aWNlUkdCCi9EZWNvZGVQYXJtcyA8PCAvQ29sb3JzIDMgL0NvbHVtbnMgMjggL1ByZWRpY3RvciAxMCA+PiAvRmlsdGVyIC9GbGF0ZURlY29kZQovSGVpZ2h0IDI4IC9MZW5ndGggNDEgMCBSIC9TdWJ0eXBlIC9JbWFnZSAvVHlwZSAvWE9iamVjdCAvV2lkdGggMjggPj4Kc3RyZWFtCkiJ7ZS9z+lQHMcPV71EGAxSiTJILDqIEYkYDGIXCYt0aAz+FomlMXTpjIREqoPRJiFYTIYmQg0nKUkr+nIHSa/Ho8W9yx2ez/Z7++b37ek5APzwFZfLRZLk8XhUFKXb7ZIkiaLoPykmk0mappWvbDab+XzebrdxHP8b0cVioZigquput+t0Om6321rEYVagKOp0OhlhoVBIpVLBYJAgiFgsRlFUr9d7d9NsNsuyrKIoGIbd5wOBQK1WgxDetj6dTvl8/l1RAACCIKVSyePxfC/V63VRFG+6xWLxA1FryuXyS1H7p6Icx73sMT0oAEA0Gq3VahzHybJsJAmC+HSPP+A4vt1u1Tt0Xb8PLexbbarruq7rRqhpmhGORiOWZc0Gf5kVBEEYDAYQQgRBeJ7neT4SiRhVFEUhhIfDQRRFi7VeU61WV6uVJEnGHZvNZk//vI+pVCqTycTQ9fv933tM7ZuxXq8lSSqVSgiCAAC8Xu94PH7osTooh8MRCoUAANfrdb/fG3mO4yCEN+PhcNjlcl0ul/tBm4Vos9lstVoAAEEQaJq22+2apgEA0ul0Lpcz2nK53HQ6fc8nAMPh8OHpe/okZjKZh0Grb7pcLmVZxjDsfD77fD6b7YkthmEYhnmw/5p4PJ5IJBqNxsONUlW13+87nc7P5H74j/gN0CJWJgplbmRzdHJlYW0KZW5kb2JqCjQxIDAgb2JqCjUxMAplbmRvYmoKMjAgMCBvYmoKPDwgL0JpdHNQZXJDb21wb25lbnQgOCAvQ29sb3JTcGFjZSAvRGV2aWNlUkdCCi9EZWNvZGVQYXJtcyA8PCAvQ29sb3JzIDMgL0NvbHVtbnMgMjggL1ByZWRpY3RvciAxMCA+PiAvRmlsdGVyIC9GbGF0ZURlY29kZQovSGVpZ2h0IDI4IC9MZW5ndGggNDIgMCBSIC9TdWJ0eXBlIC9JbWFnZSAvVHlwZSAvWE9iamVjdCAvV2lkdGggMjggPj4Kc3RyZWFtCkiJ7c+tDYRAEIbhby8LSKgAHIEiCKECuqMGgoMecBgMDoEhazAIfkKye+KSswzJiBP36m+eZIB/tMqy7Ps+CAJOdJ7nfd+jKLpdvuhoVVWO48RxzIkCWNe1rmtmVAhh2/ajk5uGYdBaF0XBJuZ5rrXeti0Mw9sx9X0pJQBjzHmebKjv+wCu65qmiQ3Nsoy4fIB+apqGH12WhR8lRkI9z0uShBmVUrquC2AcRzb0W9d1zKgQ4jgOZtQYQ1w+QJVSSin6/ibLstI0bduWTfz3E70BK49HCwplbmRzdHJlYW0KZW5kb2JqCjQyIDAgb2JqCjIxMwplbmRvYmoKMjEgMCBvYmoKPDwgL0JpdHNQZXJDb21wb25lbnQgOCAvQ29sb3JTcGFjZSAvRGV2aWNlUkdCCi9EZWNvZGVQYXJtcyA8PCAvQ29sb3JzIDMgL0NvbHVtbnMgMjggL1ByZWRpY3RvciAxMCA+PiAvRmlsdGVyIC9GbGF0ZURlY29kZQovSGVpZ2h0IDI4IC9MZW5ndGggNDMgMCBSIC9TdWJ0eXBlIC9JbWFnZSAvVHlwZSAvWE9iamVjdCAvV2lkdGggMjggPj4Kc3RyZWFtCkiJ7dMxq6pgHMdx6xRBDUEgJBQNLRFBLq1F0BuoJYSGmhpbWhyjF+DQ5BLR0muIUIIIbIkggjQcSsiGJiHsyf+DdxDk3HPOUF250/lN+kU/iCJB/O4/j2VZAKhWq16iiqIAwGg08hJNpVKqqlqW1ev1vHSdN6DreiKR8AyNRqODwQBjrCjKk7d8fE+dTud8PhuG4ZwihG63W61Wi8fj+/3etm2SJBFCCKEXHk0URVVVW62WW7LZ7GKxAACMMQAAwHg8fkEkCKJer1uWZZqmruvlcpmmaUEQHMtBLcvSNC2dTr/mNhoNV5Fl2TmWJEnTNOejvcY5CwQCzWYTY2zbNsZYkiSWZUOhEEVRq9Xq8XgMh8N3XIIgcrnc4XAAAI7j3NjtdhFCANBut990k8nk5XIxTbNUKrmR53kAOJ1Ob6IEQfT7fcuyttstSZJOiUQik8kEAJbLZTAYfNNdr9cAUKlU3ELTtGEYGGOGYb5f738GDYfDPp/vfr+7ZbPZHI9Hv9//Ob62YrE4nU5jsdjnyDDM9XqVZTmfz7/p/rj5fA4As9nsS//h339+mUymUChQFKWq6m63+xfqr3EcZ5omz/Oeic4EQRBF0WP0d1/2B+yeDG4KZW5kc3RyZWFtCmVuZG9iago0MyAwIG9iago0NjYKZW5kb2JqCjIgMCBvYmoKPDwgL0NvdW50IDEgL0tpZHMgWyAxMCAwIFIgXSAvVHlwZSAvUGFnZXMgPj4KZW5kb2JqCjQ0IDAgb2JqCjw8IC9DcmVhdGlvbkRhdGUgKEQ6MjAxODA0MTAxNTIxMDgrMDInMDAnKQovQ3JlYXRvciAobWF0cGxvdGxpYiAyLjEuMiwgaHR0cDovL21hdHBsb3RsaWIub3JnKQovUHJvZHVjZXIgKG1hdHBsb3RsaWIgcGRmIGJhY2tlbmQgMi4xLjIpID4+CmVuZG9iagp4cmVmCjAgNDUKMDAwMDAwMDAwMCA2NTUzNSBmIAowMDAwMDAwMDE2IDAwMDAwIG4gCjAwMDAwMTE2ODMgMDAwMDAgbiAKMDAwMDAwNDgzOCAwMDAwMCBuIAowMDAwMDA0ODcwIDAwMDAwIG4gCjAwMDAwMDQ5NjkgMDAwMDAgbiAKMDAwMDAwNDk5MCAwMDAwMCBuIAowMDAwMDA1MDExIDAwMDAwIG4gCjAwMDAwMDAwNjUgMDAwMDAgbiAKMDAwMDAwMDQwMSAwMDAwMCBuIAowMDAwMDAwMjA4IDAwMDAwIG4gCjAwMDAwMDA5NTggMDAwMDAgbiAKMDAwMDAwNTE0MyAwMDAwMCBuIAowMDAwMDA1ODQ3IDAwMDAwIG4gCjAwMDAwMDY1NjUgMDAwMDAgbiAKMDAwMDAwNzI1NiAwMDAwMCBuIAowMDAwMDA3ODE0IDAwMDAwIG4gCjAwMDAwMDg0OTIgMDAwMDAgbiAKMDAwMDAwOTI4MCAwMDAwMCBuIAowMDAwMDA5NzU2IDAwMDAwIG4gCjAwMDAwMTA1MTIgMDAwMDAgbiAKMDAwMDAxMDk3MSAwMDAwMCBuIAowMDAwMDAzNjU5IDAwMDAwIG4gCjAwMDAwMDM0NTkgMDAwMDAgbiAKMDAwMDAwMzEwOSAwMDAwMCBuIAowMDAwMDA0NzEyIDAwMDAwIG4gCjAwMDAwMDA5NzggMDAwMDAgbiAKMDAwMDAwMTI5OCAwMDAwMCBuIAowMDAwMDAxNDYwIDAwMDAwIG4gCjAwMDAwMDE4NTMgMDAwMDAgbiAKMDAwMDAwMjAwNSAwMDAwMCBuIAowMDAwMDAyMDk0IDAwMDAwIG4gCjAwMDAwMDI1MDUgMDAwMDAgbiAKMDAwMDAwMjgyNiAwMDAwMCBuIAowMDAwMDA1ODI3IDAwMDAwIG4gCjAwMDAwMDY1NDUgMDAwMDAgbiAKMDAwMDAwNzIzNiAwMDAwMCBuIAowMDAwMDA3Nzk0IDAwMDAwIG4gCjAwMDAwMDg0NzIgMDAwMDAgbiAKMDAwMDAwOTI2MCAwMDAwMCBuIAowMDAwMDA5NzM2IDAwMDAwIG4gCjAwMDAwMTA0OTIgMDAwMDAgbiAKMDAwMDAxMDk1MSAwMDAwMCBuIAowMDAwMDExNjYzIDAwMDAwIG4gCjAwMDAwMTE3NDMgMDAwMDAgbiAKdHJhaWxlcgo8PCAvSW5mbyA0NCAwIFIgL1Jvb3QgMSAwIFIgL1NpemUgNDUgPj4Kc3RhcnR4cmVmCjExODk3CiUlRU9GCg==\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAABQCAYAAABF70F6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAGQhJREFUeJztnXtcVWW6x38vCmggEBcFR4UpNUNSIi3lcAAnDcv7cSQd0eCM5ugnLc+ZUifzEtMHy0zFSyOOQ6OSyvGWlGYdMFRIjik5VKJJ5iVF/SC3kIS99nP+2OzV3psdF/fe6zLzfD+f5w/WWnutH+/7rPWs913P+76CiMAwDMMwWsdNbQEMwzAM0xY4YDEMwzC6gAMWwzAMows4YDEMwzC6gAMWwzAMows4YDEMwzC6gAMWwzAMowt0FbCEEN8LIeqFED822Sdqa7JECBEmhDgihLgjhCgVQgxXW5MtQog4IQQJIf6sthZLhBCpQogSIYRBCLFMbT22CCGihRD/J4SoFUL8QwgRo7YmABBCdBVC7BBCXBNCVAshCoQQT6ityxId1O0RIcQtIUSNEOKMEGKc2pos0Xr5Aco9V3QVsJoYQ0TeTfaU2mJs2AGgGEAAgFcB7BZCBKkr6WeEEO4A1gIoUluLHS4AeAXAR2oLsUUI4Q/gAICVAPwAvAUgRwhxv6rCTHgDOAngMQD+AP4O4CMhhLeqqqzRbN028SKAECLyAfA8gO1CiBCVNVmi6fJT8rmix4ClSYQQfQFEAVhKRPVEtAdACYCJ6iqz4r8BfAKgVG0hthDR34noEIBatbXYIRrADSL6HyKSiGg7gFsA/kNlXSCi74joHSK63qQtA4AHgIfU1mZG43ULIvoHERnMfwJwB9BTRUlWaL38oOBzRY8BK6up+f6JEGKg2mIs6A/gOyKydKozTdtVRwgRCuA/AbyuthYdIprMdluEClpaRAgRCVPAuqC2Fj0hhPhQCPETTK2EzwB8oa4ifaD0c0VvAWsqgDAAoQCOADgshPBTVdHPeAOottlWDaCLClrskQ7gNSL6UW0hOqQQQHchxBQhhLsQ4jkADwK4T2VdVgghfABsA7CciGx9kWkBIhoN0736DIDDRGRUWZJeUPS5oquARUQFTd1td4goDUAVgH9XW1cTPwLwsdnmAw0044UQYwB0IaJdamvRI0RUAWAcgP8CcAPASAD/C+CqmrosEUJ0BpAD4ETTvcG0EyJqbOp6SxBCjFVbj9ZR47nSUakLuQhC864atfgawANCiC4W3YIDAbyvoiYzTwIYJIQob/rbF4AkhHiEiDSVEaVViCgfwGAAEEJ0BFAGYJWqopoQQngC2A/gBwCzVJbzz0BHmFrQTMso/lzRTQtLCNFLCPFvQggPIUQnIcTLAAIBFKitDQCI6DyALwEsbdI3AcAAAHvUVQYAeA1AXwCRTXYAwGYAKWqKsqSpq60TTD7ZsakMO6ity4wQ4tEmjT4A3gZwlYgOa0CXO4DdAOoBTNdiV5aW61YI0U8I8bQQonOTziQAsQDy1dZmRsPlp/xzhYh0YTAlL/wDQB2ACgC5AAaprctGYxhMH2zrAZwDMFxtTb+g8z0Af1Zbhx1NZGPJauuy0LcDpm+S1QB2AeiqtqYmXXFNZXUHpm5ps/272tr0ULcAHoYp0aIWpk8MJwFMUFuXXsrPjk6XPldE04UYhmEYRtPopkuQYRiG+deGAxbDMAyjCzhgMQzDMLqAAxbDMAyjCzhgMQzDMLpA0YHDQghNpCQSkd3BxlrWp2VtAOtrK3rUp2VtAOtrK3qsW1u4hcUwDMPoAg5YDMMwjC7ggMUwDMPoAg5YLuCxxx5DZmYmMjMzIUkSMjMzERUVpbYshlGctWvXgohQUlKC0NBQteUwKpKbm4u8vDzHTqLwXFO282G12Tp06ED+/v6yLVmyhJYsWUIrVqygffv2Uffu3en9998nM/X19bR06VK753KFPrNFRkbS7du3yWAwWFlFRUWbz+EqbS3Zk08+SeXl5VReXk4PPfRQu7S5Ut/ixYtJkiQiIoqLi6O4uLh2l50S5edI3TpTX5cuXSgkJIRCQkJoxowZtGjRIvL09FTc98LCwigsLIwqKipIkiQyGAyUkJCgmbLr27cv9e3bl/r370+zZ88mIiJJkuza3r17ycPDgzw8PBSvW3d3d4qLi6OCggKX+J6z/K4lW716Na1evZrq6+tp06ZN7So7W9Pk8iK9evWCh4cHoqOjERMTAwDw8/PDxIn2V5u/evUq0tPTMWHCBNTWmlb2OHPmDPLzlZ1w+fHHH8eePXvg6+trdgbU1taioaEBAQEBGDJkCE6fPg0AaGhocPh6sbGxCAgIwL59+xw+1+DBg3Hy5EmHz+NMkpOTsWDBAhiNpgnIed7LXyYsLAwLFizA0KFDERFhvRBySEgI5s2bp6ieW7duAQCOHj2KsWO1s7RU//79kZycjEmTJgEA3Nzc0L17dxiNxl/0r7Fjx+Ivf/kLAOCll15CTU2NYnp9fX1x5MgRlJeXIzg4GOXl5a3/SEOsWLECf/jDHwAAjY2NyM3Ndeh8mgpYkZGRAIC8vDz4+vq26TdGoxGLFy/Gjz/+iKysLFy/fh0AUFlZiXPnzrlMq5n77jMtOhsVFYXt27cjJCTEav+3336Lt956Czt37kRBQQEWL14MAEhLc3yNvfj4ePTp08fhgOXm5oZf//rXcpeNENpYYiw0NBSdOnVSWwaeeOIJJCUlIS4uDv3795e3//GPf8S1a9cQExOD7du3o6ioSFFd/fr1A2B6iE6dOhWdO3eGEAJXrlwBYHpZevjhh5GYmIiNGzeitLRUMW11dXUAgEuXLil2zbaQlpaGZ555pt2/mz59OgBgy5YtKChQfkWj4OBgXQasIUOGwN3dHQBw/PhxZGdnO3Q+/obFMAzD6AJNtbAuX74MAKioqPjFFlZRURGqqqowbNgwAKautW3btimm0ZZNmzYBAKZMmWJ3f1RUFLy9vZGfn4/4+HgMGDDAadeePn06Pv/8c4fPExISgpkzZ2L79u0AoOibuD2GDx8OAJg7dy4Ak57Ro0fjxo0biup49tlnAZgSBwIDAyGEwGeffYagoCAAwMqVKwGYWqRBQUGYPHmyIrp8fX3x5ptvyvq6dOki7/v222+RkJAAAHB3d0dpaSkCAwMRGBioiDYzfn5+AICBAwcqet3W+PTTT61aWDdv3sSWLVvg5uYmdz0DQHR0NOLi4tSQaBet9HpYEhsbi1dffRVTpkzB7du3m+2fMmUKIiIiUFZWBsDUI+EomgpY5n/65ZdfxujRo1FcXIz09HR5/5dffokRI0agrq5O7pp58cUXVdEKmLIBR40aBeBnh8rPz0dOTg7efvttAMC1a9dQXFyMyspK/OY3v3Gq47m5OaeB/Ne//hWA6WGnNjExMcjMzAQA+aVl5cqVinYtdezYEYMGDcLmzZsBmLp9jx49itTUVBw/fhyenp4AgOzsbDz11FMAgC+++EIxfRMmTMCMGTOabS8rK8OIESPkLsHevXsrpskWc1d5r1695G2DBw+WX4bU6ip89913sX//fvnvxsZGu91sPj4++Oqrr9C9e3cAkH+jZD1bQkSa6B63JCMjA3369EF4eDiOHz/ebP+f/vQnBAQEYObMmQBMeQUOo9UsQR8fHxJCUEZGBmVkZJAkSTRlyhSnZK04Q5+9bMCcnBzy9vamUaNG0aJFi2jRokUUFBQk/0aSJKqtraXa2lqKiopyKJtnwIABVFdXR9u2bXO4PAoLC8loNNKQIUNoyJAhLi+7lmzz5s1WGVq5ubmK121ycrJVvR46dIh8fHzk/UlJSZSUlCTvv3TpklU9u1rfRx99ZKXvwoULtGPHDurVq5fVcWPGjJGPiYmJuWd9jtTna6+9JmcJGgwGeuGFF+iFF164p3MpmQE6adIkqq2tlXWvWbOG1qxZo7i+wMBAMhqNZDQa77ncXFW3p0+fJoPBQMOHD2+2LzIykmpqauTndmvP7jbHEK0GLLOtXLmSVq5cSZIkUV5eHrm5uTnsjI7q69u3L2VlZZEkSXTjxg26ceMGffnll/Tb3/62xd9Z3rhZWVkOOdbChQvJaDQ6HLC6detG169fJ6PRSD179qSePXu6tOxassDAQJIkiRobG6mxsZFu3bpFw4YNU7RuU1NT5XpKT0+n9PR0q2AFgM6ePUtnz56V63LcuHGK6QNA3bt3p2XLllF0dDRFR0dT165d7R43Y8YM1QMWbPxe6wFr8uTJNHnyZMrNzbV6KfDx8WnmB0ro8/Pzo8rKSjIajbR69WqH/jdnaUtNTaXU1FRqbGykkpKSZi9rXl5etGPHDjIYDFRQUEDu7u7k7u5+T2Vna5x0wTAMw+gDrbewvLy8yMvLi/Ly8kiSJHrqqaccfou6V32enp7k6elJBw4cIIPBQFVVVZSQkEAJCQkUEBBAPXr0aPH3lm+ax44dc+hNKDMzk4xGIy1cuNChsti2bRsZjUYqLS0lPz8/8vPzc0nZtWZhYWF06tQpqxbWkiVLFKtb80B0SZKovr6e9u/fT507d6bOnTvLx3Tq1InGjh1LdXV1VFdXR5Ik0fLlyxXRdy+2ZcsWTbSwiH4elKvVFtbUqVOppKSE6uvrqb6+3qp19cUXXzTzBSX1HThwQDMtrJ49e8oTDNTX19sdyL9p0yYyGAx0+fJlh+vW1jSVdGEP83iOmTNn4vTp09i8eTOOHDkif/zcsGGDueBdzqOPPgoAcpbRuHHjFB+cbEt7B/v6+Phg5MiRAICkpCQ5aSA1NRVVVVVO19dWRo4cKWdQmgcXrl27VpFr+/n5Yc6cOQAAIsLhw4cxfvx4q2N69+6NrKwsPPbYY/K23bt346233lJEY2vMmzcPXl5eVtseeeQRAEBhYaFTsknvlZYG5SpNWFgYpk2bJmeimomJiWmmsaamBgsXLsTBgwdRX1+vpExNEhERgX379skZp+vWrbN6/pmzAJOTkwEAb7zxhtM1aD5gmSkrK0NycjIyMzMxbdo0TJs2DQDg5eWFrVu3ygOGXck777wDwJQRmJ+f3+5gZZs66wz8/f2bbRs4cCCEEPJN2aNHD3h4eGDq1Klwc3OTb76ioiLcvXsXHTt2xKlTp5yqqz2MHz8eK1asAGAaXPjcc88BAKqrqxW5voeHh1Xa97x589C1a1ekpKQAMM10EBERAW9vb8s3U2zfvl1+oVKa++67D+Hh4QCApUuXyi9Rtj527do1pKSkQJIkVXRqiYiICBw4cMAqc7Eljh07hoyMDBerajsBAQGKX7NjR1OISEpKapb+P3ToUCxatAjvvPMO/P395dlDhBDYunWrPOTHqWi9S9DWIiIi6JNPPrHKJNu4cSP96le/cmnXwujRo+nOnTt0584dMhgM9NJLL7Vbu2WX4Lp16xxqum/cuJEkSaKKigoqLi62MkmSyGg0UkNDAzU0NFBVVRUVFhbS6tWraerUqdSjRw/q0aMHubu7040bN6ihocGlZfdLZp5vzrIuMzMznd7t0Zo+Pz8/un79Ol2/fp0MBoNVPZnt8uXLdOXKFTIYDPKxSumzNHd3d3r88cdlLQaDgWpra+nKlSuUnZ1NNTU1VrqvX79OL7/8cqvz4LXH9xzxezW7BCMiIujixYt25wu07La0tKefflqVe8PSzF2CVVVVTr83WvuNbUasJEl07tw5OnfunLztxIkTVv54L/fGP02XoC1fffUVEhMTMWbMGABAZmYmZs2ahT59+mDEiBEuu27nzp3h4eEBwDTYcNeuXW3+raenJ5YtWwYA8mzFixYtckjPnDlzcOnSJURHRzfbd/nyZezfvx9nz54FAJw4ccLuOZ5//nkEBQXhu+++c0jLvbJgwQIAsGoRmFtaSlJVVSV3AX744Yfw9/dHWVkZPvjgAwDAe++9h9u3b2Pnzp0ICQnBzp07Fddo9r2RI0di7969AIDly5cDMPlUQUEB/P39kZeXZzWXYFBQENLS0mSfAIC7d+8qqt3yrTw2NhYAsH79ekU1AKZnR3x8PJKSknD48GEAwE8//WR1zO9//3sAPw9a1wJHjhzB6NGjFb/us88+K4+JbGxsRFVVFX73u9+hsrISALBq1SrExcVh0KBBEELIPQ+BgYG4cuUK4uPj5UHDzoKzBBmGYRh9oLcuQVu7e/cuSZJEd+/epfj4eJc13SdNmiQ3eS9evNhmfZ6enpSamioPMDVnFbZXnyvKbteuXWQ0GunNN990uOne3mtHRkZSWVkZlZWVyVmBu3fvdvh/cpXvxcbGyt1Gc+fOpblz5yqmz93dndLS0igtLc1qkLplVmdQUBCdPHlSznKsr6+n5cuX0549e+TffPzxx/Txxx/TsGHDKDIykiIjIxXxPXtdrOHh4ZqpW0vz9fUlX19fWacWugQnTpxIRqOR6urqKDQ0lEJDQ53mey0dn5eXJ9+jKSkpzfaHh4fTsWPH7Hajb9261Sl120yv3gLWgAED6PXXX6dDhw7RoUOH5L7m4uLiNg8qdjRgrV27ttVrmB8IWVlZZDAYaM+ePS5zrHs1c8BqbXYLR8vOnt28eVMOVI2NjXTs2DHy9vZ2+H9yle8lJCTIN2VQUFCbZ7ZwVF+HDh1oxYoVsu9VV1fT7Nmz6f7775ePGTRoEJ04cYIMBgOVlpbSsGHD5AHXPj4+NHLkSNq2bRtVV1dTdXV1iy9erii7DRs2NAtYrc0aoWTdWlpiYiIlJiZqKmCNGzeOjEYj3blzR17Hy1m+19LxL774YouTCcTGxlJlZSUZDAZKTEyk8PBw2VobZN3WstNtwHrooYdo3bp19MMPPzT7MNrQ0EAHDx50qWMlJibK17t06VKL558/fz7dvn2bbt++TZIkOeVtw1Gnt2dqBizL8VaNjY2amnarJc1KB6zZs2eTwWCgmpoaqqmpocmTJ5O/vz89/fTTlJ2dTdnZ2fIUQkuWLGlxphLzFDk5OTmUk5NDvXv3VsT35s6dq1rAcnd3p1GjRtGoUaNaHUeVkpIil7OWAhYA+uabb8hoNNLGjRtp48aNTvO9ezmPuRW6bt06kiSJzp8/7/D/908RsIKDg2n+/Pk0f/58Kisrs5vFU1RURGPHjnW541u2sO7evUvp6ekUGRkpv4FMmjSJDhw4QJcuXSJJkujixYt08eJF2rFjR7sCgjMdqzXbtWsXERFNnz7dpWVna5mZmURknZV1r90crtBnz9RqYZkzF82DlU+dOkWlpaXNAsDixYupQ4cOLik/Z9TL+fPnm2XlPfjggy4tu5iYGDp06JBcRr8UzP39/SkpKUluLVhmX7ZnajBXBqw1a9ZQdXU1derUiTp16qRq3ZrnSTVnBLY2YYIjZWdrmswS7NatG8LDw7F+/Xp5kTpLioqK5KUdPvjgA6ePbWqNDh06YM6cOZg4caK8+mifPn3k/YWFhThy5AgAYMmSJYpqay9E5LRZ39tCZGQkhg8fDqPRKK+6vGHDBsWXDmkvDzzwgCrXLS8vR1BQkDxDvHm5joMHD+Lo0aMATDOJf//995oea/X1119blaES9+z69eutMiZfeeUVeUVyS0aMGIGoqCjzAxwA8Nlnn+Hdd9+V72MtQEROWancEUJDQ+WVAogIGRkZuHr1qmLX5yxBhmEYRhdopoXl7+8vj4yOjIy0+0ZbWFiIVatW4fDhw4pPlfL555/L0yANHjwYgGnZ6m7dusnHVFRUYOfOnaqu0XUvDB06FO+9954i1/Lz80NwcDAA4IcffgDgnIXdXM2xY8dcMlNJa8TGxmL8+PGIiooCYBoD+Le//Q2VlZWqv223h4yMDHnspFrMnj27xf03b95ETk4OANM6e7ZjtNTGx8cH48aNAwDs27dPFQ2ffvopQkNDAZhmelm6dKmyAtT+hvXEE0/Q7t276fLly3a/UZnXj3rjjTfIy8vL4b5SONDXHBISQiEhIbRs2bJmqZyrVq2y+xHbWfqccV5bM3/D2rRpk8vLzmzx8fHU2NhIkiTRhQsX6MKFC079n1z5HeH8+fNkMBjatG6YGvq07HuhoaFUUlJiNWOCq79hRUZGWk0AbM/OnTtHxcXFlJ6eThEREZr1vWvXrlF9fT3169eP+vXrp1rdmr9dGQwGmjBhgkv9zq5etQPWihUrmgWpkpISSktLo9TU1DbNIK4lx3KVPldcJzk5mYxGo6IBKzg4mPLz83UZsMwLO+bm5lJubq5mxxLpwfeUqltPT0+aNWsWzZo1i27dukUGg4F2794tbwsODtaF7+3cuZPOnDmj6DgsLdStrYkm0YoghFDuYi1ARHbXqdeyPi1rA/419Pn4+CA7O1ueVHjv3r1ISUlp1wS4eiw/LWsDWF9b0WPd2sJJFwzDMIwu4BaWBVrWp2VtwL+OPh8fH3mdn9mzZ2PAgAH45ptvNKPPUdj37h096tOyNntwwLJAy/q0rA1gfW1Fj/q0rA1gfW1Fj3Vri6IBi2EYhmHuFf6GxTAMw+gCDlgMwzCMLuCAxTAMw+gCDlgMwzCMLuCAxTAMw+gCDlgMwzCMLuCAxTAMw+gCDlgMwzCMLuCAxTAMw+gCDlgMwzCMLuCAxTAMw+gCDlgMwzCMLuCAxTAMw+gCDlgMwzCMLuCAxTAMw+gCDlgMwzCMLuCAxTAMw+gCDlgMwzCMLuCAxTAMw+gCDlgMwzCMLuCAxTAMw+gCDlgMwzCMLuCAxTAMw+iC/wc7i9gVeTIrxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2af4f80fe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data preparation\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# The data, shuffled and split between train and test sets\n",
    "(x_train_in, y_train_in), (x_test_in, y_test_in) = mnist.load_data()\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x_train_in[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\" {}\".format(y_train_in[i]))\n",
    "\n",
    "x_train_in = x_train_in.astype('float32')\n",
    "x_test_in = x_test_in.astype('float32')\n",
    "x_train_in /= 255\n",
    "x_test_in /= 255\n",
    "    \n",
    "print(x_train_in.shape, 'train samples')\n",
    "print(x_test_in.shape, 'test samples')\n",
    "\n",
    "# build training set\n",
    "x_train = np.zeros((TRAINING_SIZE, num_images,img_cols * img_rows), dtype=np.float)\n",
    "y_train = np.zeros(TRAINING_SIZE)\n",
    "for j in range(TRAINING_SIZE):    \n",
    "\n",
    "    for i in range(num_images):\n",
    "        #x_temp = x_train_in[np.random.randint(0, TRAINING_SIZE)] # select a random image\n",
    "        index = (num_images*j)+i\n",
    "        x_temp = x_train_in[index] # select next image\n",
    "        x_temp = x_temp.reshape(1, img_cols * img_rows) # flatten the image\n",
    "        x_train[j][i]= x_temp\n",
    "        y_train[j] += y_train_in[index]\n",
    "\n",
    "# build test set\n",
    "x_test = np.zeros((TESTING_SIZE, num_images, img_cols * img_rows), dtype=np.float)\n",
    "y_test = np.zeros(TESTING_SIZE)\n",
    "for j in range(TESTING_SIZE):    \n",
    "    \n",
    "    for i in range(num_images):\n",
    "        #x_temp = x_train_in[np.random.randint(0, TRAINING_SIZE)] # select a random image\n",
    "        index = (num_images*j)+i\n",
    "        x_temp = x_test_in[index] # select next image\n",
    "        x_temp = x_temp.reshape(1, img_cols * img_rows) # flatten the image\n",
    "        x_test[j][i] = x_temp\n",
    "        y_test[j]+= y_test_in[index]\n",
    "    \n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_test.shape, 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_train = keras.utils.to_categorical(y_train, 90)\n",
    "y_test = keras.utils.to_categorical(y_test, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9dfdae982317>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 5% scoring model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mRNN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mLAYERS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# need to do 9 additions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "# 5% scoring model\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1 # need to do 9 additions\n",
    "MAXLEN = 10\n",
    "\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "\n",
    "# use convolution to look at each image seperately, 10 digits -> 10 dimensions?\n",
    "model.add(Conv1D(10,img_cols * img_rows, strides=img_cols * img_rows,padding='same',input_shape=x_train.shape[1:])) \n",
    "\n",
    "#model.add(Dense(HIDDEN_SIZE, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))        \n",
    "model.add(Dense(10, activation='relu'))\n",
    "#model.add(RNN(HIDDEN_SIZE, input_shape=(10,30)))\n",
    "          \n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "#model.add(Dense(10, activation='softmax')) # 10 digits\n",
    "model.add(Flatten())\n",
    "model.add(layers.RepeatVector(10)) # do this for all 10 digits?\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    #model.add(Dense(128, activation='relu'))\n",
    "    model.add(RNN(128, return_sequences=True))\n",
    "    \n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(90))) # maximum sum\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(90, activation='relu'))\n",
    "model.add(Dense(1, activation=K.relu))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_71 (Dense)             (None, 10, 784)           615440    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 10, 784)           0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 10, 128)           100480    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 10, 10)            1290      \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_15 (RepeatVect (None, 2, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 2, 128)            117248    \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 2, 10)             1290      \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 90)                1890      \n",
      "=================================================================\n",
      "Total params: 837,638\n",
      "Trainable params: 837,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 5% scoring model\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1 # need to do 9 additions\n",
    "MAXLEN = 10\n",
    "\n",
    "\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "\n",
    "# use convolution to look at each image seperately, 10 digits -> 10 dimensions?\n",
    "#model.add(Conv1D(10,img_cols * img_rows, strides=img_cols * img_rows,padding='same',input_shape=x_train.shape[1:])) \n",
    "\n",
    "#model.add(Dense(HIDDEN_SIZE, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(784, input_shape=(10,784), activation='relu'))\n",
    "model.add(Dropout(0.2))    # lowered dropout to reduce overfitting\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(10, activation='sigmoid')) # get a value between 1 - 10 for each of the 10 images\n",
    "#model.add(RNN(HIDDEN_SIZE, input_shape=(10,30)))\n",
    "          \n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "#model.add(Dense(10, activation='softmax')) # 10 digits\n",
    "model.add(Flatten())\n",
    "model.add(layers.RepeatVector(2)) # do this for all 10 digits? or maximum length 2 (90)\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    #model.add(Dense(128, activation='relu'))\n",
    "    model.add(RNN(128, return_sequences=True))\n",
    "    \n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(10))) # need to sum 10 elements\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(90, activation='softmax'))\n",
    "#model.add(Dense(1, activation=K.relu))\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      " 396/6000 [>.............................] - ETA: 6s - loss: 0.7521 - acc: 0.7601"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test)) \n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum: \n",
      "54\n",
      "49\n",
      "sum: \n",
      "54\n",
      "53\n",
      "sum: \n",
      "48\n",
      "47\n",
      "sum: \n",
      "43\n",
      "42\n",
      "sum: \n",
      "56\n",
      "55\n",
      "sum: \n",
      "41\n",
      "42\n",
      "sum: \n",
      "42\n",
      "41\n",
      "sum: \n",
      "33\n",
      "33\n",
      "sum: \n",
      "38\n",
      "41\n",
      "sum: \n",
      "34\n",
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"\n",
      "c:\\users\\bramv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "predicted_classes = model.predict_classes(x_test)\n",
    "\n",
    "# Check which items we got right / wrong\n",
    "\n",
    "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]\n",
    "\n",
    "for i in range(10):   \n",
    "    x = np.zeros((1,10,784))\n",
    "    index = np.random.randint(0, TESTING_SIZE-1)\n",
    "    x[0] = x_test[index]\n",
    "    preds = model.predict(x, verbose=0)\n",
    "    print(\"sum: \")\n",
    "    print(predicted_classes[index])\n",
    "    print(np.argmax(y_test[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Provide your justification here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
